{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f462044-4c6e-4cc3-a38c-90f0d0b04c0d",
      "metadata": {
        "id": "4f462044-4c6e-4cc3-a38c-90f0d0b04c0d"
      },
      "source": [
        "Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f8b8cd99-3e98-4c7c-a0a6-5da590c64df2",
      "metadata": {
        "id": "f8b8cd99-3e98-4c7c-a0a6-5da590c64df2",
        "outputId": "4e59c4d0-72de-4887-ab04-54515b71b9f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /C:/ci_310/cffi_1642682485096/work\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/C:/ci_310/cffi_1642682485096/work'\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cangozpi/Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e478e929-ff08-44d9-877b-5b6cde8d49dd",
      "metadata": {
        "id": "e478e929-ff08-44d9-877b-5b6cde8d49dd"
      },
      "source": [
        "Processing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef06ac51-351f-4430-b1ed-0bddc75624b5",
      "metadata": {
        "id": "ef06ac51-351f-4430-b1ed-0bddc75624b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ea2c80-c36d-4df8-edae-20386bf907e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dropbox\n"
          ]
        }
      ],
      "source": [
        "%cd dropbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bb5fa6e2-6c2a-4a89-9135-96a1e6726ad1",
      "metadata": {
        "id": "bb5fa6e2-6c2a-4a89-9135-96a1e6726ad1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "#======================================================================================================\n",
        "### Explanation of the original .txt file: The column 'session_new_index' corresponds to user ID. \n",
        "# The column 'item_new_index' corresponds to item ID. If several items have the same 'Time' index, \n",
        "# then they are displayed at the same time (in the same display set).\n",
        "#======================================================================================================\n",
        "datasets = [\"yelp\",\"tb\",\"rsc\"]\n",
        "\n",
        "for data_set_name in datasets:\n",
        "\n",
        "\n",
        "\n",
        "  # The format of processed data:\n",
        "  # data_behavior[user][0] is user_id\n",
        "  # data_behavior[user][1][t] is displayed list at time t\n",
        "  # data_behavior[user][2][t] is picked id at time t\n",
        "\n",
        "  filename = './'+data_set_name+'.txt'\n",
        "\n",
        "  raw_data = pd.read_csv(filename, sep='\\t', usecols=[1, 3, 5, 7, 6], dtype={1: int, 3: int, 7: int, 5:int, 6:int})\n",
        "\n",
        "  raw_data.drop_duplicates(subset=['session_new_index','Time','item_new_index','is_click'], inplace=True)\n",
        "  raw_data.sort_values(by='is_click',inplace=True)\n",
        "  raw_data.drop_duplicates(keep='last', subset=['session_new_index','Time','item_new_index'], inplace=True)\n",
        "\n",
        "  sizes = raw_data.nunique()\n",
        "  size_user = sizes['session_new_index']\n",
        "  size_item = sizes['item_new_index']\n",
        "\n",
        "  data_user = raw_data.groupby(by='session_new_index')\n",
        "  data_behavior = [[] for _ in range(size_user)]\n",
        "\n",
        "  train_user = []\n",
        "  vali_user = []\n",
        "  test_user = []\n",
        "\n",
        "  sum_length = 0\n",
        "  event_cnt = 0\n",
        "\n",
        "  for user in range(size_user):\n",
        "    data_behavior[user] = [[], [], []]\n",
        "    data_behavior[user][0] = user\n",
        "    data_u = data_user.get_group(user)\n",
        "    split_tag = list(data_u['tr_val_tst'])[0]\n",
        "    if split_tag == 0:\n",
        "      train_user.append(user)\n",
        "    elif split_tag == 1:\n",
        "      vali_user.append(user)\n",
        "    else:\n",
        "      test_user.append(user)\n",
        "\n",
        "    data_u_time = data_u.groupby(by='Time')\n",
        "    time_set = np.array(list(set(data_u['Time'])))\n",
        "    time_set.sort()\n",
        "\n",
        "    true_t = 0\n",
        "    for t in range(len(time_set)):\n",
        "      display_set = data_u_time.get_group(time_set[t])\n",
        "      event_cnt += 1\n",
        "      sum_length += len(display_set)\n",
        "\n",
        "      data_behavior[user][1].append(list(display_set['item_new_index']))\n",
        "      data_behavior[user][2].append(int(display_set[display_set.is_click==1]['item_new_index']))\n",
        "\n",
        "  new_features = np.eye(size_item) # one hot encoding of unique items\n",
        "\n",
        "  filename = './'+data_set_name+'.pkl'\n",
        "  file = open(filename, 'wb')\n",
        "  pickle.dump(data_behavior, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(new_features, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  file.close()\n",
        "\n",
        "  filename = './'+data_set_name+'-split.pkl'\n",
        "  file = open(filename, 'wb')\n",
        "  pickle.dump(train_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(vali_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(test_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30190157-a3b4-43f4-99d3-6e8310655a95",
      "metadata": {
        "id": "30190157-a3b4-43f4-99d3-6e8310655a95"
      },
      "source": [
        "Pytorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV3y0EgKg9ac",
        "outputId": "54c9e93a-2ada-4813-e12d-a71e3e87222a"
      },
      "id": "mV3y0EgKg9ac",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "755ae154-90e5-4373-b77c-fe73375d0f4c",
      "metadata": {
        "id": "755ae154-90e5-4373-b77c-fe73375d0f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632a6f6a-ec9b-4738-8369-cb9a3f35b00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders successfully instantiated !\n",
            "\n",
            "=======\n",
            "Train DataLoader: \n",
            "\t\n",
            "<class 'tuple'>\n",
            "(PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([16,  1]), sorted_indices=tensor([ 1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), unsorted_indices=tensor([ 1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])), PackedSequence(data=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), batch_sizes=tensor([16,  1]), sorted_indices=tensor([ 1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), unsorted_indices=tensor([ 1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])), PackedSequence(data=tensor([0., 2., 0., 0., 3., 0., 0., 3., 3., 1., 9., 2., 2., 1., 3., 3., 4.]), batch_sizes=tensor([16,  1]), sorted_indices=tensor([ 1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), unsorted_indices=tensor([ 1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])))\n",
            "\n",
            "=======\n",
            "Validation DataLoader: \n",
            "\t\n",
            "<class 'tuple'>\n",
            "(PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([16,  3]), sorted_indices=tensor([ 3, 10, 13,  0,  1,  2,  4,  5,  6,  7,  8,  9, 11, 12, 14, 15]), unsorted_indices=tensor([ 3,  4,  5,  0,  6,  7,  8,  9, 10, 11,  1, 12, 13,  2, 14, 15])), PackedSequence(data=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), batch_sizes=tensor([16,  3]), sorted_indices=tensor([ 3, 10, 13,  0,  1,  2,  4,  5,  6,  7,  8,  9, 11, 12, 14, 15]), unsorted_indices=tensor([ 3,  4,  5,  0,  6,  7,  8,  9, 10, 11,  1, 12, 13,  2, 14, 15])), PackedSequence(data=tensor([1., 0., 3., 0., 1., 0., 1., 4., 1., 5., 0., 3., 2., 5., 2., 5., 1., 1.,\n",
            "        4.]), batch_sizes=tensor([16,  3]), sorted_indices=tensor([ 3, 10, 13,  0,  1,  2,  4,  5,  6,  7,  8,  9, 11, 12, 14, 15]), unsorted_indices=tensor([ 3,  4,  5,  0,  6,  7,  8,  9, 10, 11,  1, 12, 13,  2, 14, 15])))\n",
            "\n",
            "=======\n",
            "Test DataLoader: \n",
            "\t\n",
            "<class 'tuple'>\n",
            "(PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([16,  4]), sorted_indices=tensor([ 1,  4,  8, 11,  0,  2,  3,  5,  6,  7,  9, 10, 12, 13, 14, 15]), unsorted_indices=tensor([ 4,  0,  5,  6,  1,  7,  8,  9,  2, 10, 11,  3, 12, 13, 14, 15])), PackedSequence(data=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), batch_sizes=tensor([16,  4]), sorted_indices=tensor([ 1,  4,  8, 11,  0,  2,  3,  5,  6,  7,  9, 10, 12, 13, 14, 15]), unsorted_indices=tensor([ 4,  0,  5,  6,  1,  7,  8,  9,  2, 10, 11,  3, 12, 13, 14, 15])), PackedSequence(data=tensor([2., 1., 1., 1., 1., 0., 4., 4., 0., 2., 2., 0., 2., 9., 4., 0., 4., 9.,\n",
            "        3., 6.]), batch_sizes=tensor([16,  4]), sorted_indices=tensor([ 1,  4,  8, 11,  0,  2,  3,  5,  6,  7,  9, 10, 12, 13, 14, 15]), unsorted_indices=tensor([ 4,  0,  5,  6,  1,  7,  8,  9,  2, 10, 11,  3, 12, 13, 14, 15])))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:174: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn   \n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pickle\n",
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "class Dataset(nn.Module):\n",
        "    def __init__(self, data_folder, dset, split=\"train\"):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            data_folder (str): location of the datasset folder.\n",
        "            dset (str): type of the dataset to be used. Can be \"yelp\", \"rsc\", \"tb\"\n",
        "            split (str): can be \"train\", \"validation\", or \"test\". Determines the returned dataset split. \n",
        "        \"\"\"\n",
        "        assert split in [\"train\", \"test\", \"validation\"]\n",
        "        data_folder = \"./dropbox\"\n",
        "\n",
        "        data_filename = os.path.join(data_folder, dset+'.pkl')\n",
        "        f = open(data_filename, 'rb')\n",
        "        data_behavior = pickle.load(f)\n",
        "        item_features = pickle.load(f)\n",
        "        f.close()\n",
        "        \n",
        "        # Load user splits\n",
        "        filename = os.path.join(data_folder, dset+'-split.pkl')\n",
        "        pkl_file = open(filename, 'rb')\n",
        "        train_users = pickle.load(pkl_file)\n",
        "        val_users = pickle.load(pkl_file)\n",
        "        test_users = pickle.load(pkl_file)\n",
        "        pkl_file.close()\n",
        "\n",
        "        # data_behavior[user][0] is user_id\n",
        "        # data_behavior[user][1][t] is displayed list at time t\n",
        "        # data_behavior[user][2][t] is picked id at time t\n",
        "\n",
        "        num_items = len(item_features[0])\n",
        "\n",
        "        self.clicked_items_index_per_user = [] # --> [user, num_time_steps]\n",
        "        self.picked_item_features_per_user = [] # --> [user, num_time_steps, feature_dim]\n",
        "        self.display_set_features_per_user = [] # --> [user, num_time_steps, num_displayed_items, feature_dim]\n",
        "        \n",
        "        users = []\n",
        "        if split == \"train\":\n",
        "            users = train_users\n",
        "            \n",
        "        elif split == \"validation\":\n",
        "            users = val_users\n",
        "        else: # test split\n",
        "            users = test_users\n",
        "\n",
        "        max_display_set_features_length = 0 # will be used to pad display_set_features length to this value to have a tensor\n",
        "        for u in users:\n",
        "\n",
        "            # create clicked item (real user click) history in terms of its feature representation (dim = feature_dim)\n",
        "            picked_item_features = [] # --> [num_time_steps, features]\n",
        "            for picked_item_id in data_behavior[u][2]:\n",
        "                picked_item_features.append(item_features[picked_item_id])\n",
        "            self.picked_item_features_per_user.append(picked_item_features)\n",
        "\n",
        "            # create display_set history\n",
        "            # convert displayed item indices to corresponding item features\n",
        "            displayed_item_features_per_time = [] # --> [num_time_steps, num_displayed_items, feature_dim]\n",
        "            clicked_items_per_time = [] # --> [num_time_steps]\n",
        "            for t, displayed_item_ids in enumerate(data_behavior[u][1]): # index on time\n",
        "                # displayed_item_ids = [num_displayed_item]\n",
        "                cur_disp_features_list = [] # --> [num_displayed_items, feature_dim]\n",
        "                for index, id in enumerate(displayed_item_ids): # index on ids in the given displayed_items\n",
        "                    # create clicked item history in terms of its index in the display_set\n",
        "                    if id == data_behavior[u][2][t]:\n",
        "                        clicked_items_per_time.append(index)\n",
        "\n",
        "                    # id = int\n",
        "                    feature_vec = item_features[id]\n",
        "                    cur_disp_features_list.append(feature_vec)\n",
        "                displayed_item_features_per_time.append(cur_disp_features_list)\n",
        "                if len(cur_disp_features_list) > max_display_set_features_length:\n",
        "                    max_display_set_features_length = len(cur_disp_features_list)\n",
        "            \n",
        "            self.clicked_items_index_per_user.append(clicked_items_per_time)\n",
        "            self.display_set_features_per_user.append(displayed_item_features_per_time)\n",
        "            \n",
        "        # Pad the display_set\n",
        "        display_set_feature_dim = len(self.display_set_features_per_user[0][0][0]) # --> [user, num_time_steps, num_displayed_items, feature_dim]\n",
        "        temp_display_set_features_per_user = deepcopy(self.display_set_features_per_user)\n",
        "        for u_index, u in enumerate(temp_display_set_features_per_user):  # index on user\n",
        "            for t_index, t in enumerate(u): #index on num_time_steps (time)\n",
        "                if len(t) < max_display_set_features_length:\n",
        "                    diff = max_display_set_features_length - len(t)\n",
        "                    non_clickable_placeholder_vec = np.ones(display_set_feature_dim) # Note that we use ones vector as a placeholder for non_displayed items (padded)\n",
        "                    for i in range(diff):\n",
        "                        self.display_set_features_per_user[u_index][t_index].append(non_clickable_placeholder_vec)\n",
        "            \n",
        "            \n",
        "        # print(len(self.clicked_items_index_per_user) , \"\\t\", len(self.picked_item_features_per_user), \"\\t\", len(self.display_set_features_per_user))\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns: tuple of lists (i.e. (list, list, list, list))\n",
        "            # clicked_items --> [num_time_steps] display set index of the clicked items by the real user (gt user actions)\n",
        "            # real_click_history --> [num_time_steps, feature_dim]\n",
        "            # real_click_history_length --> [num_time_steps]\n",
        "            # display_set --> [num_time_steps, num_displayed_item, feature_dim]\n",
        "         \n",
        "        \"\"\"\n",
        "        # Note that we index on users\n",
        "        clicked_items = self.clicked_items_index_per_user[index]\n",
        "\n",
        "        real_click_history = self.picked_item_features_per_user[index] # --> [num_time_steps, picked_item_features]\n",
        "        real_click_history_length = len(real_click_history) \n",
        "        \n",
        "        display_set = self.display_set_features_per_user[index]\n",
        "\n",
        "\n",
        "        return clicked_items, real_click_history, real_click_history_length, display_set  \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.picked_item_features_per_user) # = user\n",
        "\n",
        "\n",
        "\n",
        "def custom_collate_fn(data):\n",
        "    \"\"\"\n",
        "        Used to create batches with variable sequence lengths. Output will be compatible with LSTMs.\n",
        "        --\n",
        "        Inputs: tuple of lists (i.e. (list, list, list, list))\n",
        "            # clicked_items --> [num_time_steps] display set index of the clicked items by the real user (gt user actions)\n",
        "            # real_click_history --> [num_time_steps, feature_dim]\n",
        "            # real_click_history_length --> [num_time_steps]\n",
        "            # display_set --> [num_time_steps, num_displayed_item, feature_dim]    \n",
        "\n",
        "        Returns:  tuple of torch.tensor (i.e. (torch.tensor, torch.tensor, torch.tensor))\n",
        "            # batched_clicked_items --> [batch_size (#users), max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "            # batched_real_click_history --> [batch_size (#users), max(num_time_steps), feature_dim]\n",
        "            # batched_display_set --> [batch_size (#users), max(num_time_steps), num_displayed_item, feature_dim]             \n",
        "    \"\"\"\n",
        "    # Pack Sequences here for LSTM batches with padded dimensions\n",
        "        # Record the length of every time_step\n",
        "    lengths_list = []\n",
        "    for clicked_items, real_click_history, real_click_history_length, display_set in data:\n",
        "        lengths_list.append(real_click_history_length)\n",
        "    \n",
        "    # Longest time_step length. All of the sequences will be padded towards this value\n",
        "    max_length = max(lengths_list)\n",
        "    \n",
        "    # Create the padded vectors\n",
        "    batch_size = len(data)\n",
        "    feature_dim = len(data[0][1][0])\n",
        "    num_displayed_item = len(data[0][3][0])\n",
        "\n",
        "    # Create a batch from the inputted data\n",
        "    padded_clicked_items = torch.zeros(batch_size, max_length) # --> [batch_size, max(num_time_steps)]\n",
        "    padded_real_click_history = torch.zeros(batch_size, max_length, feature_dim) # --> [batch_size, max(num_time_steps), feature_dim]\n",
        "    padded_display_set = torch.zeros(batch_size, max_length, num_displayed_item, feature_dim) # --> [batch_size, max(num_time_steps), num_displayed_item, feature_dim]\n",
        "    for i, (clicked_items, real_click_history, real_click_history_length, display_set) in enumerate(data): # index on the batch\n",
        "        # ************************ Reminder\n",
        "        # clicked_items --> [num_time_steps] display set index of the clicked items by the real user (gt user actions)\n",
        "        # real_click_history --> [num_time_steps, feature_dim]\n",
        "        # real_click_history_length --> [num_time_steps]\n",
        "        # display_set --> [num_time_steps, num_displayed_item, feature_dim] \n",
        "        # ************************\n",
        "        cur_clicked_items = torch.tensor(clicked_items) # --> [num_time_steps]\n",
        "        # print(real_click_history_length, \"\\t \", cur_clicked_items.shape)\n",
        "        padded_clicked_items[i, :real_click_history_length] = cur_clicked_items\n",
        "        \n",
        "        cur_real_click_history = torch.tensor(real_click_history) # --> [num_time_steps, feature_dim]\n",
        "        # print(real_click_history_length, \"\\t \", cur_real_click_history.shape)\n",
        "        padded_real_click_history[i, :real_click_history_length, :] = cur_real_click_history\n",
        "\n",
        "        cur_display_set = torch.tensor(display_set) # --> [num_time_steps, num_displayed_item, feature_dim]\n",
        "        # print(real_click_history_length, \"\\t \", cur_display_set.shape)\n",
        "        # print(\"True num_displayed_item = \",len(data[0][3][0]), \" cur_display_set.shape = \", cur_display_set.shape)\n",
        "        padded_display_set[i, :real_click_history_length, :, :] = cur_display_set\n",
        "\n",
        "\n",
        "    # Make padded tensors compatible with LSTMs\n",
        "    batched_click_history = torch.nn.utils.rnn.pack_padded_sequence(padded_real_click_history, lengths_list, batch_first=True, enforce_sorted = False) # --> [batch_size (#users), max(num_time_steps), feature_dim]\n",
        "    batched_display_set = torch.nn.utils.rnn.pack_padded_sequence(padded_display_set, lengths_list, batch_first=True, enforce_sorted = False) # --> [batch_size (#users), max(num_time_steps), num_displayed_item, feature_dim]\n",
        "    batched_clicked_items = torch.nn.utils.rnn.pack_padded_sequence(padded_clicked_items, lengths_list, batch_first=True, enforce_sorted = False) # --> [batch_size (#users), max(num_time_steps)]\n",
        "    \n",
        "    return batched_click_history, batched_display_set, batched_clicked_items, \n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # data_folder = \"./dropbox\"\n",
        "    # dset = \"yelp\" # choose rsc, tb, or yelp\n",
        "\n",
        "    # data_filename = os.path.join(data_folder, dset+'.pkl')\n",
        "    # f = open(data_filename, 'rb')\n",
        "    # data_behavior = pickle.load(f)\n",
        "    # item_feature = pickle.load(f)\n",
        "    # f.close()\n",
        "    # # data_behavior[user][0] is user_id\n",
        "    # # data_behavior[user][1][t] is displayed list at time t\n",
        "    # # data_behavior[user][2][t] is picked id at time t\n",
        "    # size_item = len(item_feature)\n",
        "    # size_user = len(data_behavior)\n",
        "    # f_dim = len(item_feature[0])\n",
        "\n",
        "    # # Load user splits\n",
        "    # filename = os.path.join(data_folder, dset+'-split.pkl')\n",
        "    # pkl_file = open(filename, 'rb')\n",
        "    # train_user = pickle.load(pkl_file)\n",
        "    # vali_user = pickle.load(pkl_file)\n",
        "    # test_user = pickle.load(pkl_file)\n",
        "    # pkl_file.close()\n",
        "\n",
        "    # print(\"=================================================\")\n",
        "    # print(\"size_item: \", size_item, \", size_user: \", size_user, \",data_behavior: \", np.asarray(data_behavior).shape,\\\n",
        "    #     \",item_feature: \", np.asarray(item_feature).shape, \",f_dim: \", f_dim, \\\n",
        "    #         \",train_user: \", np.asarray(train_user).shape, \", vali_user: \", np.asarray(vali_user).shape, \\\n",
        "    #             \", test_user: \", np.asarray(test_user).shape\n",
        "    #     )\n",
        "    # print(\"=================================================\")\n",
        "    # # print(np.asarray(data_behavior)[14])\n",
        "\n",
        "    # Test our custom Dataset\n",
        "    data_folder = \"./dropbox\"\n",
        "    available_datasets = [\"yelp\", \"rsc\", \"tb\"]\n",
        "    dset = available_datasets[0] # choose rsc, tb, or yelp\n",
        "    \n",
        "    # Initialize Dataloaders\n",
        "    train_dataset = Dataset(data_folder, dset, split=\"train\")\n",
        "    val_dataset = Dataset(data_folder, dset, split=\"validation\")\n",
        "    test_dataset = Dataset(data_folder, dset, split=\"test\")\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "    print(\"Dataloaders successfully instantiated !\")\n",
        "    \n",
        "    print(\"\\n=======\\nTrain DataLoader: \\n\\t\")\n",
        "    for x in train_dataloader:\n",
        "        print(type(x))\n",
        "        print(x)\n",
        "        break\n",
        "    print(\"\\n=======\\nValidation DataLoader: \\n\\t\")\n",
        "    for x in val_dataloader:\n",
        "        print(type(x))\n",
        "        print(x)\n",
        "        break\n",
        "    print(\"\\n=======\\nTest DataLoader: \\n\\t\")\n",
        "    for x in test_dataloader:\n",
        "        print(type(x))\n",
        "        print(x)\n",
        "        break\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bbfb88-5c46-4fb7-b772-79e23c77770a",
      "metadata": {
        "id": "a7bbfb88-5c46-4fb7-b772-79e23c77770a"
      },
      "source": [
        "Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "25386758-2eb1-484e-9d82-5977c3d22fa2",
      "metadata": {
        "id": "25386758-2eb1-484e-9d82-5977c3d22fa2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note that Reward Generating model is the Discriminator in this context\n",
        "class Discriminator_RewardModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, n_hidden, hidden_dim):\n",
        "        \"\"\"\n",
        "        input_size: should equal (num_displayed_items*feature_dims) + state_dim.\n",
        "        output_size: should equal (num_displayed_items+1). \n",
        "        n_hidden: number of hidden layers of the Discriminator model's MLP.\n",
        "        hidden_dim: hidden dimension of the layers of the Discriminator model's MLP.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_size = input_size\n",
        "        layers = []\n",
        "\n",
        "        layers.extend([torch.nn.Linear(input_size, hidden_dim),torch.nn.ReLU()])\n",
        "        \n",
        "        for n in range(n_hidden-1):\n",
        "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim),torch.nn.ReLU()])\n",
        "            \n",
        "            \n",
        "        # Regression Layer (outputs score for each display_item)\n",
        "        # Note that output_dim equals the number of possible actions\n",
        "        layers.extend([torch.nn.Linear(hidden_dim, output_size), torch.nn.Tanh()])\n",
        "         \n",
        "        self.model = torch.nn.Sequential(*layers) # (inp_0 inp_1 .. inp_k) --> classification(inp_0, inp_1, .. inp_k) \n",
        "\n",
        "    def forward(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            Input:\n",
        "                state (torch.Tensor): [max(num_time_steps), state_dim]\n",
        "                displayed_items (torch.Tensor): [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "            Returns:\n",
        "                reward (torch.float): reward value for taking the action at the given state. \n",
        "                [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "        \"\"\"\n",
        "        # Convert rnn.PackedSequences to simple Tensors\n",
        "        if isinstance(state, torch.nn.utils.rnn.PackedSequence):\n",
        "            state, _ = torch.nn.utils.rnn.pad_packed_sequence(state, batch_first=True)\n",
        "        if isinstance(displayed_items, torch.nn.utils.rnn.PackedSequence):\n",
        "            displayed_items, lens_displayed_item = torch.nn.utils.rnn.pad_packed_sequence(displayed_items, batch_first=True)\n",
        "        \n",
        "        # Prepare input\n",
        "        batch_size = state.shape[0] # B\n",
        "        num_time_steps = displayed_items.shape[1] # L\n",
        "        # concat zero vector to displayed items to represent user not clicking on any of the displayed items\n",
        "        not_clicking_feature_vec = torch.zeros((batch_size, num_time_steps, 1, displayed_items.shape[-1])) # --> [batch_size (#users), max(num_time_steps), 1, feature_dim]\n",
        "        displayed_items = torch.cat((displayed_items.to(self.device), not_clicking_feature_vec.to(self.device)), -2) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1), feature_dims]\n",
        "        displayed_items_flat = displayed_items.view(batch_size, num_time_steps, -1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)*feature_dims]\n",
        "        input_features = torch.cat((displayed_items_flat, state), dim=-1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items*feature_dims) + state_dim]\n",
        "        \n",
        "            \n",
        "        return self.model(input_features) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be37a83-6931-4f1e-a706-fc5d8e95410a",
      "metadata": {
        "id": "9be37a83-6931-4f1e-a706-fc5d8e95410a"
      },
      "source": [
        "Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4046a555-109f-4037-ba21-7f6813102d16",
      "metadata": {
        "id": "4046a555-109f-4037-ba21-7f6813102d16"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note that User Model is the Generator in this context\n",
        "class Generator_UserModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, n_hidden, hidden_dim):\n",
        "        \"\"\"\n",
        "        input_size: equals ((num_displayed_items+1)*feature_dims + state_dim)\n",
        "        output_size: equals (num_displayed_items+1)\n",
        "        n_hidden: number of hidden layers in the generator model.\n",
        "        hidden_dim: hidden dimension of the layers in the generator model.\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_size = input_size\n",
        "        layers = []\n",
        "        \n",
        "        layers.extend([torch.nn.Linear(input_size, hidden_dim),torch.nn.ReLU()])\n",
        "        \n",
        "        for n in range(n_hidden-1):\n",
        "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim),torch.nn.ReLU()])\n",
        "            \n",
        "            \n",
        "        # Classification Layer (outputs score for each display_item)\n",
        "        layers.extend([torch.nn.Linear(hidden_dim, output_size), torch.nn.Tanh()])\n",
        "         \n",
        "        self.model = torch.nn.Sequential(*layers) # (inp_0 inp_1 .. inp_k) --> classification(inp_0, inp_1, .. inp_k) \n",
        "                                                \n",
        "\n",
        "    def forward(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state (torch.Tensor): [batch_size (#users), num_time_steps, state_dim]\n",
        "            displayed_items (torch.Tensor): [batch_size (#users), num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            action_scores (torch.Tensor): [batch_size (#users), num_time_steps, num_displayed_items]\n",
        "        \"\"\"\n",
        "        # Convert rnn.PackedSequences to simple Tensors\n",
        "        state_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(state, batch_first=True)\n",
        "        displayed_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(displayed_items, batch_first=True)\n",
        "        \n",
        "        # Prepare input\n",
        "        batch_size = state_unpacked.shape[0] # B\n",
        "        num_time_steps = displayed_items_unpacked.shape[1] # L\n",
        "        # concat zero vector to displayed items to represent user not clicking on any of the displayed items\n",
        "        not_clicking_feature_vec = torch.zeros((batch_size, num_time_steps, 1, displayed_items_unpacked.shape[-1])) # --> [batch_size (#users), max(num_time_steps), 1, feature_dim]\n",
        "        displayed_items_unpacked = torch.cat((displayed_items_unpacked.to(self.device), not_clicking_feature_vec.to(self.device)), -2) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1), feature_dims]\n",
        "        displayed_items_flat = displayed_items_unpacked.view(batch_size, num_time_steps, -1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)*feature_dims]\n",
        "        input_features = torch.cat((displayed_items_flat, state_unpacked), dim=-1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items*feature_dims) + state_dim]\n",
        "        \n",
        "        return self.model(input_features) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "\n",
        "\n",
        "    def get_index(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state (torch.Tensor): [num_time_steps, state_dim]\n",
        "            displayed_items (torch.Tensor): [num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            generated_action_indices (torch.Tensor): [batch_size (#users), num_time_steps] indices of the actions chosen from the displayed_items by the user model\n",
        "            # Note that (num_displayed_items+1)^th index refers to the user not clickin on any of the items (i.e. zero feature vector)\n",
        "        \"\"\"\n",
        "        out = self.forward(state, displayed_items) # --> [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "        # find the action with the highest probability\n",
        "        pred_probs = torch.nn.functional.softmax(out, dim=2) # --> [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "        generated_action_indices = torch.argmax(pred_probs, dim=2) # --> [batch_size (#users), num_time_steps]\n",
        "        \n",
        "        return generated_action_indices\n",
        "\n",
        "    def get_corresponding_feature_vec(self, generated_action_indices, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            generated_action_indices (torch.Tensor): [batch_size (#users), num_time_steps] indices of the actions chosen from the displayed_items by the user model\n",
        "            # ! Note that (num_displayed_items+1)^th index refers to the user not clickin on any of the items (i.e. zero feature vector) !\n",
        "            displayed_items (torch.Tensor): [num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            generated_action_vectors (torch.Tensor): corresponding feature vectors of the generated actions specified with the generated_action_indices \n",
        "                [batch_size (#users), num_time_steps, feature_dims]\n",
        "        \"\"\"\n",
        "        # Convert rnn.PackedSequences to simple Tensors\n",
        "        displayed_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(displayed_items, batch_first=True)\n",
        "        \n",
        "        # Prepare input\n",
        "        batch_size = generated_action_indices.shape[0] # B\n",
        "        num_time_steps = displayed_items_unpacked.shape[1] # L\n",
        "        # Handle (num_displayed_items+1)^th index which refers to the user not clickin on any of the items (i.e. zero feature vector)\n",
        "        not_clicking_feature_vec = torch.zeros((batch_size, num_time_steps, 1, displayed_items_unpacked.shape[-1])).to(self.device) # --> [batch_size (#users), max(num_time_steps), 1, feature_dim]\n",
        "        displayed_items_unpacked = torch.cat((displayed_items_unpacked.to(self.device), not_clicking_feature_vec), -2) # --> [batch_size (#users), num_time_steps, (num_displayed_items+1), feature_dims]\n",
        "\n",
        "        # Extract the feature vectors that correspond to the generated action indices\n",
        "        #TODO implement this faster\n",
        "        generated_action_vectors = torch.zeros((generated_action_indices.shape[0], generated_action_indices.shape[1], displayed_items_unpacked.shape[-1])) # --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "        for b in range(generated_action_indices.shape[0]): # batch index\n",
        "            for t in range(generated_action_indices.shape[1]): # time step index\n",
        "                chosen_index = generated_action_indices[b, t]\n",
        "                generated_action_vectors[b, t, :] = displayed_items_unpacked[b, t, chosen_index, :]\n",
        "\n",
        "        return generated_action_vectors # --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "\n",
        "\n",
        "    def generate_actions(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state (torch.Tensor): [num_time_steps, state_dim]\n",
        "            displayed_items (torch.Tensor): [num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            generated_action_indices (torch.Tensor): indices of the chosen actions. [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "            generated_action_vectors (torch.Tensor): corresponding feature vectors of the generated actions specified with the generated_action_indices \n",
        "                [batch_size (#users), num_time_steps, feature_dims]\n",
        "        \"\"\"\n",
        "        # Obtain indices of the actions chosen from the display set\n",
        "        generated_action_indices = self.get_index(state, displayed_items) # --> [batch_size (#users), num_time_steps]\n",
        "        # Obtain action feature vectors corresponding to the indices of the generated actions\n",
        "        generated_action_vectors = self.get_corresponding_feature_vec(generated_action_indices, displayed_items) # --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "\n",
        "        return generated_action_indices , generated_action_vectors #[batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d7706f-1ac5-4d34-ab48-e3b8053d2373",
      "metadata": {
        "id": "17d7706f-1ac5-4d34-ab48-e3b8053d2373"
      },
      "source": [
        "HistoryLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a559c73d-2fb1-4dad-bd68-30a86ed2b9ff",
      "metadata": {
        "id": "a559c73d-2fb1-4dad-bd68-30a86ed2b9ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# This model takes in the old state and the newly chosen action as input and produces the new state representation\n",
        "# using LSTM(Long Short Term Memory)\n",
        "class History_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        \"\"\"\n",
        "        input_size (int): feature_dim of the actions.\n",
        "        hidden_size (int): dimension of the state representation vector (dim of output)\n",
        "        num_layers (int): number of recurrent layers in the LSTM model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.num_layers = num_layers\n",
        "        self.state_dim = hidden_size\n",
        "        self.lstm_model = torch.nn.LSTM(input_size, self.state_dim, self.num_layers, batch_first=True).to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, actions):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            new_action (torch.Tensor): action chosen by the user (either ground truth action or Generator_UserModel generated action).\n",
        "            [batch_size (#users), num_time_steps, feature_dim]\n",
        "        Returns:\n",
        "            new_state (torch.Tensor): old_state updated after taking new_action (i.e. updated history representation). \n",
        "            [batch_size (#users), num_time_steps, state_dim]\n",
        "            (h, c) (tuple): hidden and cell states. \n",
        "            Note that the returned new_state tensor is of same shape as the old_state tensor. \n",
        "        \"\"\"\n",
        "        out, _ = self.lstm_model(actions)\n",
        "        return out \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a471665f-b5a9-4820-9411-5e8f216dbd32",
      "metadata": {
        "id": "a471665f-b5a9-4820-9411-5e8f216dbd32"
      },
      "source": [
        "GAN Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "21a7006b-2a7c-419e-a6b1-120c91331caf",
      "metadata": {
        "id": "21a7006b-2a7c-419e-a6b1-120c91331caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9874c050-bdd9-42da-8e62-415d15726ea1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## ========================================================== DEBUG \\nif __name__ == \"__main__\":\\n    gan = GAN() #TODO: pass in constructor parameters\\n    gan.gan_training_loop(train_loader, validation_loader)\\n    # gan.test() #TODO\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from distutils.command.config import config\n",
        "import torch\n",
        "import numpy as np\n",
        "# import custom models\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_results(dreal_losses, dfake_losses, val_dreal_losses, val_dfake_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1, len(dreal_losses)+1)), dreal_losses, marker='o', label=\"dreal_loss\")\n",
        "    plt.plot(list(range(1, len(dfake_losses)+1)), dfake_losses, marker='*', label=\"dfake_loss\")\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(\"results/Training Losses Plot\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1, len(val_dreal_losses)+1)), val_dreal_losses, marker='o', label=\"dreal_loss\")\n",
        "    plt.plot(list(range(1, len(val_dfake_losses)+1)), val_dfake_losses, marker='*', label=\"dfake_loss\")\n",
        "    plt.title(\"Validation Losses\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(\"results/Validation Losses Plot\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Note that GAN is a model which orchestrated the mini-max game (training) between the  discriminator and the  generator model.\n",
        "class GAN():\n",
        "    def __init__(self, config_dict, history_input_size, history_hidden_size, history_num_layers, \\\n",
        "        generator_input_size, generator_output_size, generator_n_hidden, generator_hidden_dim, \\\n",
        "            discriminator_input_size, discriminator_output_size, discriminator_n_hidden, discriminator_hidden_dim, \\\n",
        "                lr=0.0006, betas=[0.3,0.999], epochs=150):        \n",
        "        \"\"\"\n",
        "        == Parameters of the History_LSTM:\n",
        "            history_input_size (int): feature_dim of the actions.\n",
        "            history_hidden_size (int): dimension of the state representation vector (dim of output of the History_LSTM)\n",
        "            history_num_layers (int): number of recurrent layers in the History_LSTM.\n",
        "\n",
        "        == Parameters of the Generator_UserModel:\n",
        "            generator_input_size (int): equals ((num_displayed_items+1)*feature_dims + state_dim)\n",
        "            generator_output_size (int): equals (num_displayed_items+1)\n",
        "            generator_n_hidden (int): number of hidden layers in the generator model.\n",
        "            generator_hidden_dim (int): hidden dimension of the layers in the generator model.\n",
        "\n",
        "        == Parameters of teh Discriminator_RewardModel:\n",
        "            discriminator_input_size (int): should equal (num_displayed_items*feature_dims) + state_dim.\n",
        "            discriminator_output_size (int): should equal (num_displayed_items+1). \n",
        "            discriminator_n_hidden (int): number of hidden layers of the Discriminator model's MLP.\n",
        "            discriminator_hidden_dim (int): hidden dimension of the layers of the Discriminator model's MLP.\n",
        "        \n",
        "        == Hyperparameters of the training\n",
        "            lr (int): learning rate used by the optimizer.\n",
        "            betas (tuple): beta values used by the ADAM optimizer.\n",
        "            epochs (int): number of epochs to train.\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.history_LSTM = History_LSTM(history_input_size, history_hidden_size, history_num_layers).to(self.device)\n",
        "        self.generator_UserModel = Generator_UserModel(generator_input_size, generator_output_size, generator_n_hidden, generator_hidden_dim).to(self.device)\n",
        "        self.discriminator_RewardModel = Discriminator_RewardModel(discriminator_input_size, discriminator_output_size, discriminator_n_hidden, discriminator_hidden_dim).to(self.device)\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.epochs = epochs\n",
        "        self.config_dict = config_dict\n",
        "        \n",
        "    \n",
        "    def gan_training_loop(self, train_loader, validation_loader):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            train_loader (torch.Tensor): training DataLoader\n",
        "            test_loader (torch.Tensor): training DataLoader \n",
        "        Return:\n",
        "            generated_actions (torch.tensor): Actions taken by the generator_UserModel.\n",
        "            UserModel_rewards (torch.tensor): Reward values for the generator_UserModel generated actions.\n",
        "            ground_truth_rewards (torch.tensor): Reward values for the ground truth actions.\n",
        "        \"\"\"\n",
        "        history_LSTM_optimizer = torch.optim.Adam(self.history_LSTM.parameters(), lr=self.lr, betas=self.betas)\n",
        "        discriminator_optimizer = torch.optim.Adam(self.discriminator_RewardModel.parameters(), lr=self.lr, betas=self.betas)\n",
        "        generator_optimizer = torch.optim.Adam(self.generator_UserModel.parameters(), lr=self.lr, betas=self.betas)\n",
        "\n",
        "\n",
        "        # ============= Load models from ckpts\n",
        "        loaded_epoch = 0\n",
        "        dreal_loaded_loss = None\n",
        "        dfake_loaded_loss = None\n",
        "        if self.config_dict[\"load_pretrained\"]:\n",
        "            history_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_history_lstm_path\"]))\n",
        "            generator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_generator_path\"]))\n",
        "            discriminator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_discriminator_path\"]))\n",
        "\n",
        "            self.history_LSTM.load_state_dict(history_ckpt[\"state_dict\"])\n",
        "            self.generator_UserModel.load_state_dict(generator_ckpt[\"state_dict\"])\n",
        "            self.discriminator_RewardModel.load_state_dict(discriminator_ckpt[\"state_dict\"])\n",
        "\n",
        "            history_LSTM_optimizer.load_state_dict(history_ckpt[\"optimizer_state_dict\"])\n",
        "            discriminator_optimizer.load_state_dict(discriminator_ckpt[\"optimizer_state_dict\"])\n",
        "            generator_optimizer.load_state_dict(generator_ckpt[\"optimizer_state_dict\"])\n",
        "\n",
        "            loaded_epoch = generator_ckpt[\"epoch\"]\n",
        "            dreal_loaded_loss = generator_ckpt[\"dreal_loss\"]\n",
        "            dfake_loaded_loss = generator_ckpt[\"dfake_loss\"]\n",
        "            print(f\"Loaded History_lstm, Discriminator, and Generator from saved ckpt. Loaded epoch:{loaded_epoch}, \\\n",
        "                Loaded best real validation loss: {dreal_loaded_loss}, Loaded best fkae validation loss: {dfake_loaded_loss}\")\n",
        "        # ================\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize empty lists to hold the generator and discriminator losses\n",
        "        dfake_losses = [] # training losses\n",
        "        dreal_losses = []\n",
        "        val_dfake_losses = [] # validation losses\n",
        "        val_dreal_losses = []\n",
        "\n",
        "        print(\"*\" * 30)\n",
        "        print(\"Training GAN Model\")\n",
        "        print(\"*\" * 30)\n",
        "\n",
        "        for epoch in range(self.epochs - loaded_epoch): \n",
        "            dreal_best_val_loss = None if dreal_loaded_loss == None else dreal_loaded_loss # best validation loss (used during saving checkpoints)\n",
        "            dfake_best_val_loss = None if dfake_loaded_loss == None else dfake_loaded_loss # best validation loss (used during saving checkpoints)\n",
        "            cur_dreal_loss = 0 # total loss for cur batch\n",
        "            cur_dfake_loss = 0 # total loss for cur batch\n",
        "            for real_click_history, display_set, clicked_items  in train_loader:\n",
        "                # real_click_history --> [max(num_time_steps), feature_dim]\n",
        "                # display_set --> [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "                # clicked_items --> [max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "                 \n",
        "                real_click_history = real_click_history.to(self.device)\n",
        "                display_set = display_set.to(self.device)\n",
        "                clicked_items = clicked_items.to(self.device)\n",
        "\n",
        "                # Updating the discriminator, here is a pseudocode        \n",
        "                # call zero grad\n",
        "                # pass the real actions through D\n",
        "                # calculate d_real loss\n",
        "                # generate fake user actions\n",
        "                # pass the generated_user_actions through D\n",
        "                # calculate d_fake loss\n",
        "                # sum the two losses\n",
        "                # call backward and take optimizer step\n",
        "\n",
        "\n",
        "\n",
        "                # ************************************ discriminator_RewardModel Loss Calculation below: ************************************\n",
        "\n",
        "                # Obtain state representations given the real user's past click history\n",
        "                real_states = self.history_LSTM(real_click_history) # --> [batch_size (#users)=1, num_time_steps, state_dim]\n",
        "                # Calculate the rewards for all of the possible actions (items in the (display_set+1))\n",
        "                dreal_reward = self.discriminator_RewardModel.forward(real_states, display_set) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                \n",
        "                # Calculate the rewards for the real user actions by masking by the actions taken by the real user\n",
        "                class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                clicked_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                clicked_item_mask = torch.nn.functional.one_hot(clicked_items_unpacked.long(), num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                gt_reward = dreal_reward * clicked_item_mask.float()\n",
        "                _, total_unpadded_num_time_steps = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                dreal_loss = torch.sum(gt_reward) / sum(total_unpadded_num_time_steps) # avg loss/rewards for the real user actions (gt)\n",
        "\n",
        "\n",
        "\n",
        "                # ========== generator_UserModel Loss Calculation below: \n",
        "                # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                with torch.no_grad():\n",
        "                    generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                # convert rnn.PackedSequence to Tensor\n",
        "                real_click_history_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                    for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                        cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                        cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                        # append generated action to past history from the real user\n",
        "                        cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                        cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                        # obtain new state representations after taking the current generated action\n",
        "                        cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "\n",
        "                        # calculate the reward for the currently generated action\n",
        "                        display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                        cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                        # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                        cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                        \n",
        "                        class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                        cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        \n",
        "                        cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                \n",
        "                dfake_loss = gen_reward # total loss/rewards for the real user actions (gt)\n",
        "\n",
        "                # Update Disciriminator (Reward) model\n",
        "                # ============ loss backpropagation:\n",
        "                combined_loss = dfake_loss - dreal_loss\n",
        "                if combined_loss.requires_grad:\n",
        "                    # Backprop discriminator_RewardModel\n",
        "                    # Note that discriminator_RewardModel tries to minimize the combined_loss\n",
        "                    for param in self.discriminator_RewardModel.parameters():\n",
        "                        param.requires_grad = True\n",
        "                    for param in self.generator_UserModel.parameters():\n",
        "                        param.requires_grad = False\n",
        "                    history_LSTM_optimizer.zero_grad()\n",
        "                    generator_optimizer.zero_grad()\n",
        "                    discriminator_optimizer.zero_grad()\n",
        "                    combined_loss.backward()\n",
        "                    history_LSTM_optimizer.step()\n",
        "                    discriminator_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # ************************************ generator_UserModel Loss Calculation below: ************************************\n",
        "                # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                # convert rnn.PackedSequence to Tensor\n",
        "                # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                    for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                        cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                        cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                        # append generated action to past history from the real user\n",
        "                        cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                        cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                        # obtain new state representations after taking the current generated action\n",
        "                        cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "                        # calculate the reward for the currently generated action\n",
        "                        display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                        \n",
        "                        cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                        # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                        cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                        \n",
        "                        class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                        clicked_item_mask = torch.nn.functional.one_hot(clicked_items.long().data, num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                        cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        \n",
        "                        cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                \n",
        "                dfake_loss = -1 * gen_reward # total loss/rewards for the real user actions (gt)\n",
        "                \n",
        "                # ============ loss backpropagation:\n",
        "                combined_loss = dfake_loss\n",
        "                if combined_loss.requires_grad:\n",
        "                    # backprop generator_UserModel\n",
        "                    # Note that generator_UserModel tries to maximize the combined_loss\n",
        "                    for param in self.generator_UserModel.parameters():\n",
        "                        param.requires_grad = True\n",
        "                    for param in self.discriminator_RewardModel.parameters():\n",
        "                        param.requires_grad = False\n",
        "                    history_LSTM_optimizer.zero_grad()\n",
        "                    generator_optimizer.zero_grad()\n",
        "                    discriminator_optimizer.zero_grad()\n",
        "                    combined_loss.backward()\n",
        "                    history_LSTM_optimizer.step()\n",
        "                    generator_optimizer.step()\n",
        "\n",
        "                    # record losses\n",
        "                    cur_dfake_loss += dfake_loss.detach().cpu().numpy()\n",
        "                    cur_dreal_loss += dreal_loss.detach().cpu().numpy()\n",
        "\n",
        "            # logging\n",
        "            dreal_losses.append(cur_dreal_loss)\n",
        "            dfake_losses.append(cur_dfake_loss)\n",
        "\n",
        "        \n",
        "\n",
        "            # ================== Validation part\n",
        "            val_cur_dreal_loss = 0 # total loss for cur batch\n",
        "            val_cur_dfake_loss = 0 # total loss for cur batch\n",
        "            for real_click_history, display_set, clicked_items  in validation_loader:\n",
        "                # real_click_history --> [max(num_time_steps), feature_dim]\n",
        "                # display_set --> [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "                # clicked_items --> [max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "                \n",
        "                real_click_history = real_click_history.to(self.device)\n",
        "                display_set = display_set.to(self.device)\n",
        "                clicked_items = clicked_items.to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                     # Obtain state representations given the real user's past click history\n",
        "                    real_states = self.history_LSTM(real_click_history) # --> [batch_size (#users)=1, num_time_steps, state_dim]\n",
        "                    # Calculate the rewards for all of the possible actions (items in the (display_set+1))\n",
        "                    dreal_reward = self.discriminator_RewardModel.forward(real_states, display_set) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                    \n",
        "                    # Calculate the rewards for the real user actions by masking by the actions taken by the real user\n",
        "                    class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                    clicked_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                    clicked_item_mask = torch.nn.functional.one_hot(clicked_items_unpacked.long(), num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                    gt_reward = dreal_reward * clicked_item_mask.float()\n",
        "                    dreal_loss = torch.sum(gt_reward) / dreal_reward.shape[1] # avg loss/rewards for the real user actions (gt)\n",
        "\n",
        "\n",
        "\n",
        "                    # ========== generator_UserModel Loss Calculation below: \n",
        "                    # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                    generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                    # convert rnn.PackedSequence to Tensor\n",
        "                    real_click_history_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                    # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                    gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                    for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                        for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                            cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                            cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                            # append generated action to past history from the real user\n",
        "                            cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                            cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                            # obtain new state representations after taking the current generated action\n",
        "                            cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "                            # calculate the reward for the currently generated action\n",
        "                            display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                            cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                            cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                            # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                            cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                            \n",
        "                            class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                            cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                            \n",
        "                            cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                            gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                    \n",
        "                    dfake_loss = -1 * gen_reward # total loss/rewards for the real user actions (gt)\n",
        "\n",
        "                    # record losses\n",
        "                    val_cur_dfake_loss += dfake_loss.detach().cpu().numpy()\n",
        "                    val_cur_dreal_loss += dreal_loss.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "            # =========== Save Checkpoints\n",
        "            if (dfake_best_val_loss == None) or (dfake_best_val_loss >= val_cur_dfake_loss):\n",
        "                if not os.path.exists(self.config_dict[\"ckpt_path\"]):\n",
        "                    os.mkdir(self.config_dict[\"ckpt_path\"])\n",
        "\n",
        "                dfake_best_val_loss = val_cur_dfake_loss\n",
        "                # Save history_lstm\n",
        "                torch.save({\n",
        "                    'epoch': epoch + loaded_epoch,\n",
        "                    'state_dict': self.history_LSTM.state_dict(),\n",
        "                    'optimizer_state_dict': history_LSTM_optimizer.state_dict(),\n",
        "                    'dfake_loss': dfake_best_val_loss,\n",
        "                    'dreal_loss': val_cur_dreal_loss,\n",
        "                }, os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_history_lstm_path\"]))\n",
        "\n",
        "                # Save Generator\n",
        "                torch.save({\n",
        "                    'epoch': epoch + loaded_epoch,\n",
        "                    'state_dict': self.generator_UserModel.state_dict(),\n",
        "                    'optimizer_state_dict': generator_optimizer.state_dict(),\n",
        "                    'dfake_loss': dfake_best_val_loss,\n",
        "                    'dreal_loss': val_cur_dreal_loss,\n",
        "                }, os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_generator_path\"]))\n",
        "\n",
        "                # Save Discriminator\n",
        "                torch.save({\n",
        "                    'epoch': epoch + loaded_epoch,\n",
        "                    'state_dict': self.discriminator_RewardModel.state_dict(),\n",
        "                    'optimizer_state_dict': discriminator_optimizer.state_dict(),\n",
        "                    'dreal_loss': dfake_best_val_loss,\n",
        "                    'dreal_loss': val_cur_dreal_loss,\n",
        "                }, os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_discriminator_path\"]))\n",
        "\n",
        "                print(\"*\" * 20)\n",
        "                print(f\"Saved model checkpoint at epoch: {epoch+loaded_epoch}\")\n",
        "            \n",
        "            # logging\n",
        "            val_dreal_losses.append(val_cur_dreal_loss)\n",
        "            val_dfake_losses.append(val_cur_dfake_loss)\n",
        "\n",
        "            print(\"_\" * 25)\n",
        "            print(f\"epoch: [{epoch+1+loaded_epoch}/{self.epochs}], train_dreal_loss: {dreal_losses[-1]}, train_dfake_loss: {dfake_losses[-1]} \\\n",
        "                val_dreal_loss: {val_dreal_losses[-1]}, val_dfake_loss: {val_dfake_losses[-1]}\")\n",
        "            print(\"_\" * 25)\n",
        "\n",
        "        plot_results(dreal_losses, dfake_losses, val_dreal_losses, val_dfake_losses)\n",
        "        # Return the losses\n",
        "        return dreal_losses, dfake_losses, val_dreal_losses, val_dfake_losses\n",
        "\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        print(\"*\" * 30)\n",
        "        print(\"Testing GAN Model\")\n",
        "        print(\"*\" * 30)\n",
        "\n",
        "        # ================== Load ckpt\n",
        "        if self.config_dict[\"load_pretrained\"]:\n",
        "            history_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_history_lstm_path\"]))\n",
        "            generator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_generator_path\"]))\n",
        "            discriminator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_discriminator_path\"]))\n",
        "\n",
        "            self.history_LSTM.load_state_dict(history_ckpt[\"state_dict\"])\n",
        "            self.generator_UserModel.load_state_dict(generator_ckpt[\"state_dict\"])\n",
        "            self.discriminator_RewardModel.load_state_dict(discriminator_ckpt[\"state_dict\"])\n",
        "\n",
        "           \n",
        "            loaded_epoch = generator_ckpt[\"epoch\"]\n",
        "            dreal_loaded_loss = generator_ckpt[\"dreal_loss\"]\n",
        "            dfake_loaded_loss = generator_ckpt[\"dfake_loss\"]\n",
        "            print(f\"Loaded History_lstm, Discriminator, and Generator from saved ckpt. Loaded epoch:{loaded_epoch}, \\\n",
        "                Loaded best real validation loss: {dreal_loaded_loss}, Loaded best fake validation loss: {dfake_loaded_loss}\")\n",
        "        # ==================\n",
        "\n",
        "        top_k_precisions_list = [] # top k@precision \n",
        "        generator_precision = [[]] # only top 1@prec scoker\n",
        "        for k in self.config_dict[\"k\"]: # initialize list\n",
        "            top_k_precisions_list.append(list())\n",
        "                \n",
        "        test_cur_dreal_loss = 0 # total loss for cur batch\n",
        "        test_cur_dfake_loss = 0 # total loss for cur batch\n",
        "        \n",
        "        for real_click_history, display_set, clicked_items  in test_dataloader:\n",
        "            # real_click_history --> [max(num_time_steps), feature_dim]\n",
        "            # display_set --> [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "            # clicked_items --> [max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "            \n",
        "            real_click_history = real_click_history.to(self.device)\n",
        "            display_set = display_set.to(self.device)\n",
        "            clicked_items = clicked_items.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Obtain state representations given the real user's past click history\n",
        "                real_states = self.history_LSTM(real_click_history) # --> [batch_size (#users)=1, num_time_steps, state_dim]\n",
        "                # Calculate the rewards for all of the possible actions (items in the (display_set+1))\n",
        "                dreal_reward = self.discriminator_RewardModel.forward(real_states, display_set) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                \n",
        "                \n",
        "                # ===============\n",
        "                # Find unpadded indices of the displayed_set\n",
        "                unpacked_displayed_items, lens_displayed_item = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                unpacked_displayed_items = unpacked_displayed_items.to(self.device)\n",
        "                lens_displayed_item = lens_displayed_item.to(self.device)\n",
        "                # unpacked_displayed_items --> [B, max(num_time_steps), padded_display_set, feature_dim]\n",
        "                display_unpadded_indices = []\n",
        "                for b in range(unpacked_displayed_items.shape[0]):\n",
        "                    cur_l_indices = []\n",
        "                    for l in range(unpacked_displayed_items.shape[1]):\n",
        "                        cur_t_indices = []\n",
        "                        for i, item in enumerate(unpacked_displayed_items[b,l, :]):\n",
        "                            # item --> [feature_dim]\n",
        "                            if item.sum() != item.shape[-1]: # input is a padding vector for displayed_items (full 1 Tensor [1,1,1 ..., 1])\n",
        "                                cur_t_indices.append(i)\n",
        "                        cur_t_indices.append(len(unpacked_displayed_items[b,l, :])) # append non_clicking vector\n",
        "                        cur_l_indices.append(cur_t_indices)\n",
        "                                \n",
        "                    display_unpadded_indices.append(cur_l_indices)\n",
        "                \n",
        "                # display_unpadded_indices --> [B, l, value]\n",
        "\n",
        "\n",
        "                # Find max k indices which are not padded in the displayed_items\n",
        "                # dreal_reward --> [B, l, max(num_displayed_item)]\n",
        "                # unpacked_clicked_items --> [B, l]\n",
        "                # lens_clicked_item --> [l]\n",
        "                unpacked_clicked_items, lens_clicked_item = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                unpacked_clicked_items =  unpacked_clicked_items.to(self.device)\n",
        "                lens_clicked_item = lens_clicked_item.to(self.device)\n",
        "                for k in self.config_dict[\"k\"]:\n",
        "                    precision_list = []\n",
        "                    for b in range(dreal_reward.shape[0]):\n",
        "                        for l in range(lens_clicked_item[b]):\n",
        "                            # dreal_reward[b,l,:] --> 11\n",
        "                            # find max k indices\n",
        "                            unpadded_display_set = torch.gather(dreal_reward[b, l, :], 0, torch.tensor(display_unpadded_indices[b][l]).to(self.device))\n",
        "                            # chose max k from unpadded_display_set\n",
        "                            _, top_k_pred = torch.topk(unpadded_display_set, k)\n",
        "                            \n",
        "                            top_k_pred = top_k_pred.tolist()\n",
        "                            # find real user's choice index\n",
        "                            real_indices = unpacked_clicked_items[b, l]\n",
        "                            \n",
        "                            if real_indices in top_k_pred: # if the discriminator guessed right\n",
        "                                precision_list.append(1)\n",
        "                            else: # if the discriminator couldn't guess right\n",
        "                                precision_list.append(0)\n",
        "                    \n",
        "                    top_k_precisions_list[k-1].extend(precision_list)\n",
        "                # =====================\n",
        "                \n",
        "                \n",
        "\n",
        "                # Calculate the rewards for the real user actions by masking by the actions taken by the real user\n",
        "                class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                \n",
        "\n",
        "                clicked_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                clicked_item_mask = torch.nn.functional.one_hot(clicked_items_unpacked.long(), num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                gt_reward = dreal_reward * clicked_item_mask.float()\n",
        "                dreal_loss = torch.sum(gt_reward) / dreal_reward.shape[1] # avg loss/rewards for the real user actions (gt)\n",
        "\n",
        "\n",
        "\n",
        "                # ========== generator_UserModel top-k@Precision Calculation below: \n",
        "                # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                # convert rnn.PackedSequence to Tensor\n",
        "                generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                # generated_action_indices --> [B, L] index of the best chosen action\n",
        "                \n",
        "                generator_precision_list = []\n",
        "                for b in range(generated_action_indices.shape[0]):\n",
        "                    for l in range(lens_clicked_item[b]):\n",
        "                            # dreal_reward[b,l,:] --> 11\n",
        "                            # find max k indices\n",
        "                        top_1_pred = generated_action_indices[b, l]\n",
        "                        top_1_pred = [top_1_pred.tolist()]\n",
        "\n",
        "                        # find real user's choice index\n",
        "                        real_indices = unpacked_clicked_items[b, l]\n",
        "                            \n",
        "                        if real_indices in top_1_pred: # if the discriminator guessed right\n",
        "                            generator_precision_list.append(1)\n",
        "                        else: # if the discriminator couldn't guess right\n",
        "                            generator_precision_list.append(0)\n",
        "                    \n",
        "                generator_precision[0].extend(generator_precision_list)\n",
        "                \n",
        "                \n",
        "                \n",
        "                real_click_history_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                \n",
        "                \n",
        "                for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                    for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                        cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                        cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                        # append generated action to past history from the real user\n",
        "                        cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                        cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                        # obtain new state representations after taking the current generated action\n",
        "                        cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "                        # calculate the reward for the currently generated action\n",
        "                        display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                        cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                        # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                        cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                        \n",
        "                        class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                        cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        \n",
        "                        cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                \n",
        "                dfake_loss = -1 * gen_reward # total loss/rewards for the real user actions (gt)\n",
        "\n",
        "                # record losses\n",
        "                test_cur_dfake_loss += dfake_loss.detach().cpu().numpy()\n",
        "                test_cur_dreal_loss += dreal_loss.detach().cpu().numpy()\n",
        "        \n",
        "        \n",
        "        # calculate top k@prec\n",
        "        top_k_precicions = []\n",
        "        for e in top_k_precisions_list:\n",
        "            top_k_precicions.append(sum(e)/len(e))\n",
        "\n",
        "        padded_display_set_size = self.config_dict[\"generator_output_size\"]\n",
        "        print(\"*\"*10)\n",
        "        print(f\"Padded Display set size = {padded_display_set_size}\")\n",
        "        for k in self.config_dict[\"k\"]:\n",
        "            print(f\"Greedy Discriminator Reward Model Prec@{k} = {top_k_precicions[k-1]}\")\n",
        "\n",
        "        print(f\"Generator User Model Prec@1 = {np.mean(generator_precision[0])}\")\n",
        "        print(\"*\"*10)\n",
        "                \n",
        "\n",
        "        print(f\"test_cur_dfake_loss: {test_cur_dfake_loss}, test_cur_dreal_loss: {test_cur_dreal_loss}\")\n",
        "        print(\"_\" * 25)\n",
        "\n",
        "        return test_cur_dreal_loss, test_cur_dfake_loss\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## ========================================================== DEBUG \n",
        "if __name__ == \"__main__\":\n",
        "    gan = GAN() #TODO: pass in constructor parameters\n",
        "    gan.gan_training_loop(train_loader, validation_loader)\n",
        "    # gan.test() #TODO\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711611ea-480a-4669-997d-57abdb8befc3",
      "metadata": {
        "id": "711611ea-480a-4669-997d-57abdb8befc3"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14ea2aa-beda-428b-a4bd-24aa8d5f262f",
      "metadata": {
        "id": "e14ea2aa-beda-428b-a4bd-24aa8d5f262f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fbbef6-4440-419d-d152-4c5476a04da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************\n",
            "Training GAN Model\n",
            "******************************\n",
            "********************\n",
            "Saved model checkpoint at epoch: 0\n",
            "_________________________\n",
            "epoch: [1/2], train_dreal_loss: 17.342899636831135, train_dfake_loss: 372.91424149274826                 val_dreal_loss: 113.64069437980652, val_dfake_loss: 406.7583751678467\n",
            "_________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import yaml\n",
        "from copy import deepcopy\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_dataLoaders(data_folder, dset, batch_size):\n",
        "    # Initialize Dataloaders\n",
        "    train_dataset = Dataset(data_folder, dset, split=\"train\")\n",
        "    val_dataset = Dataset(data_folder, dset, split=\"validation\")\n",
        "    test_dataset = Dataset(data_folder, dset, split=\"test\")\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "config_dict = {}\n",
        "\n",
        "mode = \"train\"\n",
        "\n",
        "# history_input_size: 804 # yelp = 804, rsc = 890, tb = 4042\n",
        "config_dict[\"history_hidden_size\"]= 512\n",
        "config_dict[\"history_num_layers\"]= 8\n",
        "\n",
        "# generator_input_size: 9356  # yelp = 9356, rsc = 10302, tb = 44974\n",
        "# generator_output_size: 11\n",
        "config_dict[\"generator_n_hidden\"]= 8\n",
        "config_dict[\"generator_hidden_dim\"]= 512 \n",
        "\n",
        "# discriminator_input_size: 9356 # yelp = 9356, rsc = 10302, tb = 44974\n",
        "# discriminator_output_size: 11\n",
        "config_dict[\"discriminator_n_hidden\"]= 8\n",
        "config_dict[\"discriminator_hidden_dim\"]= 512\n",
        "\n",
        "config_dict[\"lr\"]= 0.0006\n",
        "config_dict[\"betas\"]= [0.3,0.999]\n",
        "config_dict[\"epochs\"]= 2\n",
        "config_dict[\"batch_size\"]= 16\n",
        "config_dict[\"k\"]= [1, 2] # top k@precision's k values\n",
        "\n",
        "config_dict[\"load_pretrained\"]= False # load history_lstm, generator, and discrminator from checkpoints if given True\n",
        "config_dict[\"ckpt_path\"]= \"checkpoints\" # folder path to checkpoints\n",
        "config_dict[\"pretrained_history_lstm_path\"]= \"best_lstm_ckpt.pth.tar\" # history_lstm checkpoint to save/load model\n",
        "config_dict[\"pretrained_discriminator_path\"]= \"best_discriminator_ckpt.pth.tar\" # discriminator checkpoint to save/load model\n",
        "config_dict[\"pretrained_generator_path\"]= \"best_generator_ckpt.pth.tar\" # generator checkpoint to save/load model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         == Parameters of the History_LSTM:\n",
        "#             history_input_size (int): feature_dim of the actions.\n",
        "#             history_hidden_size (int): dimension of the state representation vector (dim of output of the History_LSTM)\n",
        "#             history_num_layers (int): number of recurrent layers in the History_LSTM.\n",
        "\n",
        "#         == Parameters of the Generator_UserModel:\n",
        "#             generator_input_size (int): equals ((num_displayed_items+1)*feature_dims + state_dim)\n",
        "#             generator_output_size (int): equals (num_displayed_items+1)\n",
        "#             generator_n_hidden (int): number of hidden layers in the generator model.\n",
        "#             generator_hidden_dim (int): hidden dimension of the layers in the generator model.\n",
        "\n",
        "#         == Parameters of teh Discriminator_RewardModel:\n",
        "#             discriminator_input_size (int): should equal (num_displayed_items*feature_dims) + state_dim.\n",
        "#             discriminator_output_size (int): should equal (num_displayed_items+1). \n",
        "#             discriminator_n_hidden (int): number of hidden layers of the Discriminator model's MLP.\n",
        "#             discriminator_hidden_dim (int): hidden dimension of the layers of the Discriminator model's MLP.\n",
        "        \n",
        "#         == Hyperparameters of the training\n",
        "#             lr (int): learning rate used by the optimizer.\n",
        "#             betas (tuple): beta values used by the ADAM optimizer.\n",
        "#             epochs (int): number of epochs to train. //\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize dataloaders\n",
        "data_folder = \"./dropbox\"\n",
        "dset = \"yelp\" # choose rsc, tb, or yelp\n",
        "assert dset in [\"yelp\", \"rsc\", \"tb\"]\n",
        "train_dataloader, val_dataloader, test_dataloader = get_dataLoaders(data_folder, dset, config_dict['batch_size'])\n",
        "\n",
        "if mode == \"train\":\n",
        "  real_click_history, display_set, clicked_items = next(iter(train_dataloader))\n",
        "  display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "elif mode == \"test\":\n",
        "  real_click_history, display_set, clicked_items = next(iter(test_dataloader))\n",
        "  display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        \n",
        "config_dict[\"generator_output_size\"] = display_set_unpacked.shape[-2] + 1\n",
        "config_dict[\"discriminator_output_size\"] = display_set_unpacked.shape[-2] + 1\n",
        "config_dict[\"history_input_size\"] = display_set_unpacked.shape[-1]\n",
        "config_dict[\"generator_input_size\"] = config_dict[\"history_hidden_size\"] + (config_dict[\"generator_output_size\"] * config_dict[\"history_input_size\"])\n",
        "config_dict[\"discriminator_input_size\"] = config_dict[\"history_hidden_size\"] + (config_dict[\"discriminator_output_size\"] * config_dict[\"history_input_size\"])\n",
        "\n",
        "  # Initialize the GAN model\n",
        "gan = GAN(config_dict, config_dict['history_input_size'], config_dict['history_hidden_size'], config_dict['history_num_layers'], \\\n",
        "        config_dict['generator_input_size'], config_dict['generator_output_size'], config_dict['generator_n_hidden'], config_dict['generator_hidden_dim'], \\\n",
        "            config_dict['discriminator_input_size'], config_dict['discriminator_output_size'], config_dict['discriminator_n_hidden'], config_dict['discriminator_hidden_dim'], \\\n",
        "                lr=config_dict['lr'], betas=config_dict['betas'], epochs=config_dict['epochs'])\n",
        "\n",
        "\n",
        "    # Train/Test using the GAN model\n",
        "if mode == \"train\":\n",
        "  train_dreal_losses, train_dfake_losses, val_dreal_losses, val_dfake_losses = gan.gan_training_loop(train_dataloader, val_dataloader)\n",
        "else:\n",
        "  test_cur_dreal_loss, test_cur_dfake_loss = gan.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34686866-053e-4969-a9ef-ac9761cf495e",
      "metadata": {
        "id": "34686866-053e-4969-a9ef-ac9761cf495e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0637cddd-6857-4b2c-84d3-886ae47ca56f",
      "metadata": {
        "id": "0637cddd-6857-4b2c-84d3-886ae47ca56f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "COMP 447 Final Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}