{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f462044-4c6e-4cc3-a38c-90f0d0b04c0d",
      "metadata": {
        "id": "4f462044-4c6e-4cc3-a38c-90f0d0b04c0d"
      },
      "source": [
        "Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f8b8cd99-3e98-4c7c-a0a6-5da590c64df2",
      "metadata": {
        "id": "f8b8cd99-3e98-4c7c-a0a6-5da590c64df2",
        "outputId": "1d3d4614-15e0-456f-e672-9fc770127e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System'...\n",
            "remote: Enumerating objects: 263, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 263 (delta 54), reused 55 (delta 27), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (263/263), 400.38 KiB | 4.82 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cangozpi/Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F075_I7g0Esh",
        "outputId": "c31eea4d-b1e0-40fb-ba48-cf6689b6de18"
      },
      "id": "F075_I7g0Esh",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e478e929-ff08-44d9-877b-5b6cde8d49dd",
      "metadata": {
        "id": "e478e929-ff08-44d9-877b-5b6cde8d49dd"
      },
      "source": [
        "Processing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ef06ac51-351f-4430-b1ed-0bddc75624b5",
      "metadata": {
        "id": "ef06ac51-351f-4430-b1ed-0bddc75624b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdd4f99-e993-43c0-f3e1-3fd0ee0e0e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System/dropbox\n"
          ]
        }
      ],
      "source": [
        "%cd dropbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bb5fa6e2-6c2a-4a89-9135-96a1e6726ad1",
      "metadata": {
        "id": "bb5fa6e2-6c2a-4a89-9135-96a1e6726ad1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "#======================================================================================================\n",
        "### Explanation of the original .txt file: The column 'session_new_index' corresponds to user ID. \n",
        "# The column 'item_new_index' corresponds to item ID. If several items have the same 'Time' index, \n",
        "# then they are displayed at the same time (in the same display set).\n",
        "#======================================================================================================\n",
        "datasets = [\"yelp\",\"tb\",\"rsc\"]\n",
        "\n",
        "for data_set_name in datasets:\n",
        "\n",
        "\n",
        "\n",
        "  # The format of processed data:\n",
        "  # data_behavior[user][0] is user_id\n",
        "  # data_behavior[user][1][t] is displayed list at time t\n",
        "  # data_behavior[user][2][t] is picked id at time t\n",
        "\n",
        "  filename = './'+data_set_name+'.txt'\n",
        "\n",
        "  raw_data = pd.read_csv(filename, sep='\\t', usecols=[1, 3, 5, 7, 6], dtype={1: int, 3: int, 7: int, 5:int, 6:int})\n",
        "\n",
        "  raw_data.drop_duplicates(subset=['session_new_index','Time','item_new_index','is_click'], inplace=True)\n",
        "  raw_data.sort_values(by='is_click',inplace=True)\n",
        "  raw_data.drop_duplicates(keep='last', subset=['session_new_index','Time','item_new_index'], inplace=True)\n",
        "\n",
        "  sizes = raw_data.nunique()\n",
        "  size_user = sizes['session_new_index']\n",
        "  size_item = sizes['item_new_index']\n",
        "\n",
        "  data_user = raw_data.groupby(by='session_new_index')\n",
        "  data_behavior = [[] for _ in range(size_user)]\n",
        "\n",
        "  train_user = []\n",
        "  vali_user = []\n",
        "  test_user = []\n",
        "\n",
        "  sum_length = 0\n",
        "  event_cnt = 0\n",
        "\n",
        "  for user in range(size_user):\n",
        "    data_behavior[user] = [[], [], []]\n",
        "    data_behavior[user][0] = user\n",
        "    data_u = data_user.get_group(user)\n",
        "    split_tag = list(data_u['tr_val_tst'])[0]\n",
        "    if split_tag == 0:\n",
        "      train_user.append(user)\n",
        "    elif split_tag == 1:\n",
        "      vali_user.append(user)\n",
        "    else:\n",
        "      test_user.append(user)\n",
        "\n",
        "    data_u_time = data_u.groupby(by='Time')\n",
        "    time_set = np.array(list(set(data_u['Time'])))\n",
        "    time_set.sort()\n",
        "\n",
        "    true_t = 0\n",
        "    for t in range(len(time_set)):\n",
        "      display_set = data_u_time.get_group(time_set[t])\n",
        "      event_cnt += 1\n",
        "      sum_length += len(display_set)\n",
        "\n",
        "      data_behavior[user][1].append(list(display_set['item_new_index']))\n",
        "      data_behavior[user][2].append(int(display_set[display_set.is_click==1]['item_new_index']))\n",
        "\n",
        "  new_features = np.eye(size_item) # one hot encoding of unique items\n",
        "\n",
        "  filename = './'+data_set_name+'.pkl'\n",
        "  file = open(filename, 'wb')\n",
        "  pickle.dump(data_behavior, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(new_features, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  file.close()\n",
        "\n",
        "  filename = './'+data_set_name+'-split.pkl'\n",
        "  file = open(filename, 'wb')\n",
        "  pickle.dump(train_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(vali_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle.dump(test_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30190157-a3b4-43f4-99d3-6e8310655a95",
      "metadata": {
        "id": "30190157-a3b4-43f4-99d3-6e8310655a95"
      },
      "source": [
        "Pytorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV3y0EgKg9ac",
        "outputId": "25c62f56-7cd6-4d47-ff7e-a532aa042aa7"
      },
      "id": "mV3y0EgKg9ac",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Generative-Adversarial-User-Model-for-RL-Based-Recommendation-System\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "755ae154-90e5-4373-b77c-fe73375d0f4c",
      "metadata": {
        "id": "755ae154-90e5-4373-b77c-fe73375d0f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d7e187-cabf-4ca6-a19e-f916f0b4fa7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders successfully instantiated !\n",
            "\n",
            "=======\n",
            "Train DataLoader: \n",
            "\t\n",
            "<class 'tuple'>\n",
            "(PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([16,  2]), sorted_indices=tensor([ 5,  9,  0,  1,  2,  3,  4,  6,  7,  8, 10, 11, 12, 13, 14, 15]), unsorted_indices=tensor([ 2,  3,  4,  5,  6,  0,  7,  8,  9,  1, 10, 11, 12, 13, 14, 15])), PackedSequence(data=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), batch_sizes=tensor([16,  2]), sorted_indices=tensor([ 5,  9,  0,  1,  2,  3,  4,  6,  7,  8, 10, 11, 12, 13, 14, 15]), unsorted_indices=tensor([ 2,  3,  4,  5,  6,  0,  7,  8,  9,  1, 10, 11, 12, 13, 14, 15])), PackedSequence(data=tensor([0., 2., 1., 0., 1., 0., 2., 2., 2., 1., 3., 2., 0., 5., 1., 2., 1., 7.]), batch_sizes=tensor([16,  2]), sorted_indices=tensor([ 5,  9,  0,  1,  2,  3,  4,  6,  7,  8, 10, 11, 12, 13, 14, 15]), unsorted_indices=tensor([ 2,  3,  4,  5,  6,  0,  7,  8,  9,  1, 10, 11, 12, 13, 14, 15])))\n",
            "\n",
            "=======\n",
            "Validation DataLoader: \n",
            "\t\n",
            "<class 'tuple'>\n",
            "(PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([16,  3]), sorted_indices=tensor([ 3, 10, 13,  0,  1,  2,  4,  5,  6,  7,  8,  9, 11, 12, 14, 15]), unsorted_indices=tensor([ 3,  4,  5,  0,  6,  7,  8,  9, 10, 11,  1, 12, 13,  2, 14, 15])), PackedSequence(data=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), batch_sizes=tensor([16,  3]), sorted_indices=tensor([ 3, 10, 13,  0,  1,  2,  4,  5,  6,  7,  8,  9, 11, 12, 14, 15]), unsorted_indices=tensor([ 3,  4,  5,  0,  6,  7,  8,  9, 10, 11,  1, 12, 13,  2, 14, 15])), PackedSequence(data=tensor([1., 0., 3., 0., 1., 0., 1., 4., 1., 5., 0., 3., 2., 5., 2., 5., 1., 1.,\n",
            "        4.]), batch_sizes=tensor([16,  3]), sorted_indices=tensor([ 3, 10, 13,  0,  1,  2,  4,  5,  6,  7,  8,  9, 11, 12, 14, 15]), unsorted_indices=tensor([ 3,  4,  5,  0,  6,  7,  8,  9, 10, 11,  1, 12, 13,  2, 14, 15])))\n",
            "\n",
            "=======\n",
            "Test DataLoader: \n",
            "\t\n",
            "<class 'tuple'>\n",
            "(PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([16,  4]), sorted_indices=tensor([ 1,  4,  8, 11,  0,  2,  3,  5,  6,  7,  9, 10, 12, 13, 14, 15]), unsorted_indices=tensor([ 4,  0,  5,  6,  1,  7,  8,  9,  2, 10, 11,  3, 12, 13, 14, 15])), PackedSequence(data=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), batch_sizes=tensor([16,  4]), sorted_indices=tensor([ 1,  4,  8, 11,  0,  2,  3,  5,  6,  7,  9, 10, 12, 13, 14, 15]), unsorted_indices=tensor([ 4,  0,  5,  6,  1,  7,  8,  9,  2, 10, 11,  3, 12, 13, 14, 15])), PackedSequence(data=tensor([2., 1., 1., 1., 1., 0., 4., 4., 0., 2., 2., 0., 2., 9., 4., 0., 4., 9.,\n",
            "        3., 6.]), batch_sizes=tensor([16,  4]), sorted_indices=tensor([ 1,  4,  8, 11,  0,  2,  3,  5,  6,  7,  9, 10, 12, 13, 14, 15]), unsorted_indices=tensor([ 4,  0,  5,  6,  1,  7,  8,  9,  2, 10, 11,  3, 12, 13, 14, 15])))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:174: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn   \n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pickle\n",
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "class Dataset(nn.Module):\n",
        "    def __init__(self, data_folder, dset, split=\"train\"):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            data_folder (str): location of the datasset folder.\n",
        "            dset (str): type of the dataset to be used. Can be \"yelp\", \"rsc\", \"tb\"\n",
        "            split (str): can be \"train\", \"validation\", or \"test\". Determines the returned dataset split. \n",
        "        \"\"\"\n",
        "        assert split in [\"train\", \"test\", \"validation\"]\n",
        "        data_folder = \"./dropbox\"\n",
        "\n",
        "        data_filename = os.path.join(data_folder, dset+'.pkl')\n",
        "        f = open(data_filename, 'rb')\n",
        "        data_behavior = pickle.load(f)\n",
        "        item_features = pickle.load(f)\n",
        "        f.close()\n",
        "        \n",
        "        # Load user splits\n",
        "        filename = os.path.join(data_folder, dset+'-split.pkl')\n",
        "        pkl_file = open(filename, 'rb')\n",
        "        train_users = pickle.load(pkl_file)\n",
        "        val_users = pickle.load(pkl_file)\n",
        "        test_users = pickle.load(pkl_file)\n",
        "        pkl_file.close()\n",
        "\n",
        "        # data_behavior[user][0] is user_id\n",
        "        # data_behavior[user][1][t] is displayed list at time t\n",
        "        # data_behavior[user][2][t] is picked id at time t\n",
        "\n",
        "        num_items = len(item_features[0])\n",
        "\n",
        "        self.clicked_items_index_per_user = [] # --> [user, num_time_steps]\n",
        "        self.picked_item_features_per_user = [] # --> [user, num_time_steps, feature_dim]\n",
        "        self.display_set_features_per_user = [] # --> [user, num_time_steps, num_displayed_items, feature_dim]\n",
        "        \n",
        "        users = []\n",
        "        if split == \"train\":\n",
        "            users = train_users\n",
        "            \n",
        "        elif split == \"validation\":\n",
        "            users = val_users\n",
        "        else: # test split\n",
        "            users = test_users\n",
        "\n",
        "        max_display_set_features_length = 0 # will be used to pad display_set_features length to this value to have a tensor\n",
        "        for u in users:\n",
        "\n",
        "            # create clicked item (real user click) history in terms of its feature representation (dim = feature_dim)\n",
        "            picked_item_features = [] # --> [num_time_steps, features]\n",
        "            for picked_item_id in data_behavior[u][2]:\n",
        "                picked_item_features.append(item_features[picked_item_id])\n",
        "            self.picked_item_features_per_user.append(picked_item_features)\n",
        "\n",
        "            # create display_set history\n",
        "            # convert displayed item indices to corresponding item features\n",
        "            displayed_item_features_per_time = [] # --> [num_time_steps, num_displayed_items, feature_dim]\n",
        "            clicked_items_per_time = [] # --> [num_time_steps]\n",
        "            for t, displayed_item_ids in enumerate(data_behavior[u][1]): # index on time\n",
        "                # displayed_item_ids = [num_displayed_item]\n",
        "                cur_disp_features_list = [] # --> [num_displayed_items, feature_dim]\n",
        "                for index, id in enumerate(displayed_item_ids): # index on ids in the given displayed_items\n",
        "                    # create clicked item history in terms of its index in the display_set\n",
        "                    if id == data_behavior[u][2][t]:\n",
        "                        clicked_items_per_time.append(index)\n",
        "\n",
        "                    # id = int\n",
        "                    feature_vec = item_features[id]\n",
        "                    cur_disp_features_list.append(feature_vec)\n",
        "                displayed_item_features_per_time.append(cur_disp_features_list)\n",
        "                if len(cur_disp_features_list) > max_display_set_features_length:\n",
        "                    max_display_set_features_length = len(cur_disp_features_list)\n",
        "            \n",
        "            self.clicked_items_index_per_user.append(clicked_items_per_time)\n",
        "            self.display_set_features_per_user.append(displayed_item_features_per_time)\n",
        "            \n",
        "        # Pad the display_set\n",
        "        display_set_feature_dim = len(self.display_set_features_per_user[0][0][0]) # --> [user, num_time_steps, num_displayed_items, feature_dim]\n",
        "        temp_display_set_features_per_user = deepcopy(self.display_set_features_per_user)\n",
        "        for u_index, u in enumerate(temp_display_set_features_per_user):  # index on user\n",
        "            for t_index, t in enumerate(u): #index on num_time_steps (time)\n",
        "                if len(t) < max_display_set_features_length:\n",
        "                    diff = max_display_set_features_length - len(t)\n",
        "                    non_clickable_placeholder_vec = np.ones(display_set_feature_dim) # Note that we use ones vector as a placeholder for non_displayed items (padded)\n",
        "                    for i in range(diff):\n",
        "                        self.display_set_features_per_user[u_index][t_index].append(non_clickable_placeholder_vec)\n",
        "            \n",
        "            \n",
        "        # print(len(self.clicked_items_index_per_user) , \"\\t\", len(self.picked_item_features_per_user), \"\\t\", len(self.display_set_features_per_user))\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns: tuple of lists (i.e. (list, list, list, list))\n",
        "            # clicked_items --> [num_time_steps] display set index of the clicked items by the real user (gt user actions)\n",
        "            # real_click_history --> [num_time_steps, feature_dim]\n",
        "            # real_click_history_length --> [num_time_steps]\n",
        "            # display_set --> [num_time_steps, num_displayed_item, feature_dim]\n",
        "         \n",
        "        \"\"\"\n",
        "        # Note that we index on users\n",
        "        clicked_items = self.clicked_items_index_per_user[index]\n",
        "\n",
        "        real_click_history = self.picked_item_features_per_user[index] # --> [num_time_steps, picked_item_features]\n",
        "        real_click_history_length = len(real_click_history) \n",
        "        \n",
        "        display_set = self.display_set_features_per_user[index]\n",
        "\n",
        "\n",
        "        return clicked_items, real_click_history, real_click_history_length, display_set  \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.picked_item_features_per_user) # = user\n",
        "\n",
        "\n",
        "\n",
        "def custom_collate_fn(data):\n",
        "    \"\"\"\n",
        "        Used to create batches with variable sequence lengths. Output will be compatible with LSTMs.\n",
        "        --\n",
        "        Inputs: tuple of lists (i.e. (list, list, list, list))\n",
        "            # clicked_items --> [num_time_steps] display set index of the clicked items by the real user (gt user actions)\n",
        "            # real_click_history --> [num_time_steps, feature_dim]\n",
        "            # real_click_history_length --> [num_time_steps]\n",
        "            # display_set --> [num_time_steps, num_displayed_item, feature_dim]    \n",
        "\n",
        "        Returns:  tuple of torch.tensor (i.e. (torch.tensor, torch.tensor, torch.tensor))\n",
        "            # batched_clicked_items --> [batch_size (#users), max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "            # batched_real_click_history --> [batch_size (#users), max(num_time_steps), feature_dim]\n",
        "            # batched_display_set --> [batch_size (#users), max(num_time_steps), num_displayed_item, feature_dim]             \n",
        "    \"\"\"\n",
        "    # Pack Sequences here for LSTM batches with padded dimensions\n",
        "        # Record the length of every time_step\n",
        "    lengths_list = []\n",
        "    for clicked_items, real_click_history, real_click_history_length, display_set in data:\n",
        "        lengths_list.append(real_click_history_length)\n",
        "    \n",
        "    # Longest time_step length. All of the sequences will be padded towards this value\n",
        "    max_length = max(lengths_list)\n",
        "    \n",
        "    # Create the padded vectors\n",
        "    batch_size = len(data)\n",
        "    feature_dim = len(data[0][1][0])\n",
        "    num_displayed_item = len(data[0][3][0])\n",
        "\n",
        "    # Create a batch from the inputted data\n",
        "    padded_clicked_items = torch.zeros(batch_size, max_length) # --> [batch_size, max(num_time_steps)]\n",
        "    padded_real_click_history = torch.zeros(batch_size, max_length, feature_dim) # --> [batch_size, max(num_time_steps), feature_dim]\n",
        "    padded_display_set = torch.zeros(batch_size, max_length, num_displayed_item, feature_dim) # --> [batch_size, max(num_time_steps), num_displayed_item, feature_dim]\n",
        "    for i, (clicked_items, real_click_history, real_click_history_length, display_set) in enumerate(data): # index on the batch\n",
        "        # ************************ Reminder\n",
        "        # clicked_items --> [num_time_steps] display set index of the clicked items by the real user (gt user actions)\n",
        "        # real_click_history --> [num_time_steps, feature_dim]\n",
        "        # real_click_history_length --> [num_time_steps]\n",
        "        # display_set --> [num_time_steps, num_displayed_item, feature_dim] \n",
        "        # ************************\n",
        "        cur_clicked_items = torch.tensor(clicked_items) # --> [num_time_steps]\n",
        "        # print(real_click_history_length, \"\\t \", cur_clicked_items.shape)\n",
        "        padded_clicked_items[i, :real_click_history_length] = cur_clicked_items\n",
        "        \n",
        "        cur_real_click_history = torch.tensor(real_click_history) # --> [num_time_steps, feature_dim]\n",
        "        # print(real_click_history_length, \"\\t \", cur_real_click_history.shape)\n",
        "        padded_real_click_history[i, :real_click_history_length, :] = cur_real_click_history\n",
        "\n",
        "        cur_display_set = torch.tensor(display_set) # --> [num_time_steps, num_displayed_item, feature_dim]\n",
        "        # print(real_click_history_length, \"\\t \", cur_display_set.shape)\n",
        "        # print(\"True num_displayed_item = \",len(data[0][3][0]), \" cur_display_set.shape = \", cur_display_set.shape)\n",
        "        padded_display_set[i, :real_click_history_length, :, :] = cur_display_set\n",
        "\n",
        "\n",
        "    # Make padded tensors compatible with LSTMs\n",
        "    batched_click_history = torch.nn.utils.rnn.pack_padded_sequence(padded_real_click_history, lengths_list, batch_first=True, enforce_sorted = False) # --> [batch_size (#users), max(num_time_steps), feature_dim]\n",
        "    batched_display_set = torch.nn.utils.rnn.pack_padded_sequence(padded_display_set, lengths_list, batch_first=True, enforce_sorted = False) # --> [batch_size (#users), max(num_time_steps), num_displayed_item, feature_dim]\n",
        "    batched_clicked_items = torch.nn.utils.rnn.pack_padded_sequence(padded_clicked_items, lengths_list, batch_first=True, enforce_sorted = False) # --> [batch_size (#users), max(num_time_steps)]\n",
        "    \n",
        "    return batched_click_history, batched_display_set, batched_clicked_items, \n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # data_folder = \"./dropbox\"\n",
        "    # dset = \"yelp\" # choose rsc, tb, or yelp\n",
        "\n",
        "    # data_filename = os.path.join(data_folder, dset+'.pkl')\n",
        "    # f = open(data_filename, 'rb')\n",
        "    # data_behavior = pickle.load(f)\n",
        "    # item_feature = pickle.load(f)\n",
        "    # f.close()\n",
        "    # # data_behavior[user][0] is user_id\n",
        "    # # data_behavior[user][1][t] is displayed list at time t\n",
        "    # # data_behavior[user][2][t] is picked id at time t\n",
        "    # size_item = len(item_feature)\n",
        "    # size_user = len(data_behavior)\n",
        "    # f_dim = len(item_feature[0])\n",
        "\n",
        "    # # Load user splits\n",
        "    # filename = os.path.join(data_folder, dset+'-split.pkl')\n",
        "    # pkl_file = open(filename, 'rb')\n",
        "    # train_user = pickle.load(pkl_file)\n",
        "    # vali_user = pickle.load(pkl_file)\n",
        "    # test_user = pickle.load(pkl_file)\n",
        "    # pkl_file.close()\n",
        "\n",
        "    # print(\"=================================================\")\n",
        "    # print(\"size_item: \", size_item, \", size_user: \", size_user, \",data_behavior: \", np.asarray(data_behavior).shape,\\\n",
        "    #     \",item_feature: \", np.asarray(item_feature).shape, \",f_dim: \", f_dim, \\\n",
        "    #         \",train_user: \", np.asarray(train_user).shape, \", vali_user: \", np.asarray(vali_user).shape, \\\n",
        "    #             \", test_user: \", np.asarray(test_user).shape\n",
        "    #     )\n",
        "    # print(\"=================================================\")\n",
        "    # # print(np.asarray(data_behavior)[14])\n",
        "\n",
        "    # Test our custom Dataset\n",
        "    data_folder = \"./dropbox\"\n",
        "    available_datasets = [\"yelp\", \"rsc\", \"tb\"]\n",
        "    dset = available_datasets[0] # choose rsc, tb, or yelp\n",
        "    \n",
        "    # Initialize Dataloaders\n",
        "    train_dataset = Dataset(data_folder, dset, split=\"train\")\n",
        "    val_dataset = Dataset(data_folder, dset, split=\"validation\")\n",
        "    test_dataset = Dataset(data_folder, dset, split=\"test\")\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "    print(\"Dataloaders successfully instantiated !\")\n",
        "    \n",
        "    print(\"\\n=======\\nTrain DataLoader: \\n\\t\")\n",
        "    for x in train_dataloader:\n",
        "        print(type(x))\n",
        "        print(x)\n",
        "        break\n",
        "    print(\"\\n=======\\nValidation DataLoader: \\n\\t\")\n",
        "    for x in val_dataloader:\n",
        "        print(type(x))\n",
        "        print(x)\n",
        "        break\n",
        "    print(\"\\n=======\\nTest DataLoader: \\n\\t\")\n",
        "    for x in test_dataloader:\n",
        "        print(type(x))\n",
        "        print(x)\n",
        "        break\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bbfb88-5c46-4fb7-b772-79e23c77770a",
      "metadata": {
        "id": "a7bbfb88-5c46-4fb7-b772-79e23c77770a"
      },
      "source": [
        "Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "25386758-2eb1-484e-9d82-5977c3d22fa2",
      "metadata": {
        "id": "25386758-2eb1-484e-9d82-5977c3d22fa2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note that Reward Generating model is the Discriminator in this context\n",
        "class Discriminator_RewardModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, n_hidden, hidden_dim):\n",
        "        \"\"\"\n",
        "        input_size: should equal (num_displayed_items*feature_dims) + state_dim.\n",
        "        output_size: should equal (num_displayed_items+1). \n",
        "        n_hidden: number of hidden layers of the Discriminator model's MLP.\n",
        "        hidden_dim: hidden dimension of the layers of the Discriminator model's MLP.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_size = input_size\n",
        "        layers = []\n",
        "\n",
        "        layers.extend([torch.nn.Linear(input_size, hidden_dim),torch.nn.ReLU()])\n",
        "        \n",
        "        for n in range(n_hidden-1):\n",
        "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim),torch.nn.ReLU()])\n",
        "            \n",
        "            \n",
        "        # Regression Layer (outputs score for each display_item)\n",
        "        # Note that output_dim equals the number of possible actions\n",
        "        layers.extend([torch.nn.Linear(hidden_dim, output_size), torch.nn.Tanh()])\n",
        "         \n",
        "        self.model = torch.nn.Sequential(*layers) # (inp_0 inp_1 .. inp_k) --> classification(inp_0, inp_1, .. inp_k) \n",
        "\n",
        "    def forward(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            Input:\n",
        "                state (torch.Tensor): [max(num_time_steps), state_dim]\n",
        "                displayed_items (torch.Tensor): [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "            Returns:\n",
        "                reward (torch.float): reward value for taking the action at the given state. \n",
        "                [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "        \"\"\"\n",
        "        # Convert rnn.PackedSequences to simple Tensors\n",
        "        if isinstance(state, torch.nn.utils.rnn.PackedSequence):\n",
        "            state, _ = torch.nn.utils.rnn.pad_packed_sequence(state, batch_first=True)\n",
        "        if isinstance(displayed_items, torch.nn.utils.rnn.PackedSequence):\n",
        "            displayed_items, lens_displayed_item = torch.nn.utils.rnn.pad_packed_sequence(displayed_items, batch_first=True)\n",
        "        \n",
        "        # Prepare input\n",
        "        batch_size = state.shape[0] # B\n",
        "        num_time_steps = displayed_items.shape[1] # L\n",
        "        # concat zero vector to displayed items to represent user not clicking on any of the displayed items\n",
        "        not_clicking_feature_vec = torch.zeros((batch_size, num_time_steps, 1, displayed_items.shape[-1])) # --> [batch_size (#users), max(num_time_steps), 1, feature_dim]\n",
        "        displayed_items = torch.cat((displayed_items.to(self.device), not_clicking_feature_vec.to(self.device)), -2) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1), feature_dims]\n",
        "        displayed_items_flat = displayed_items.view(batch_size, num_time_steps, -1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)*feature_dims]\n",
        "        input_features = torch.cat((displayed_items_flat, state), dim=-1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items*feature_dims) + state_dim]\n",
        "        \n",
        "            \n",
        "        return self.model(input_features) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be37a83-6931-4f1e-a706-fc5d8e95410a",
      "metadata": {
        "id": "9be37a83-6931-4f1e-a706-fc5d8e95410a"
      },
      "source": [
        "Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4046a555-109f-4037-ba21-7f6813102d16",
      "metadata": {
        "id": "4046a555-109f-4037-ba21-7f6813102d16"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note that User Model is the Generator in this context\n",
        "class Generator_UserModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, n_hidden, hidden_dim):\n",
        "        \"\"\"\n",
        "        input_size: equals ((num_displayed_items+1)*feature_dims + state_dim)\n",
        "        output_size: equals (num_displayed_items+1)\n",
        "        n_hidden: number of hidden layers in the generator model.\n",
        "        hidden_dim: hidden dimension of the layers in the generator model.\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_size = input_size\n",
        "        layers = []\n",
        "        \n",
        "        layers.extend([torch.nn.Linear(input_size, hidden_dim),torch.nn.ReLU()])\n",
        "        \n",
        "        for n in range(n_hidden-1):\n",
        "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim),torch.nn.ReLU()])\n",
        "            \n",
        "            \n",
        "        # Classification Layer (outputs score for each display_item)\n",
        "        layers.extend([torch.nn.Linear(hidden_dim, output_size), torch.nn.Tanh()])\n",
        "         \n",
        "        self.model = torch.nn.Sequential(*layers) # (inp_0 inp_1 .. inp_k) --> classification(inp_0, inp_1, .. inp_k) \n",
        "                                                \n",
        "\n",
        "    def forward(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state (torch.Tensor): [batch_size (#users), num_time_steps, state_dim]\n",
        "            displayed_items (torch.Tensor): [batch_size (#users), num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            action_scores (torch.Tensor): [batch_size (#users), num_time_steps, num_displayed_items]\n",
        "        \"\"\"\n",
        "        # Convert rnn.PackedSequences to simple Tensors\n",
        "        state_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(state, batch_first=True)\n",
        "        displayed_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(displayed_items, batch_first=True)\n",
        "        \n",
        "        # Prepare input\n",
        "        batch_size = state_unpacked.shape[0] # B\n",
        "        num_time_steps = displayed_items_unpacked.shape[1] # L\n",
        "        # concat zero vector to displayed items to represent user not clicking on any of the displayed items\n",
        "        not_clicking_feature_vec = torch.zeros((batch_size, num_time_steps, 1, displayed_items_unpacked.shape[-1])) # --> [batch_size (#users), max(num_time_steps), 1, feature_dim]\n",
        "        displayed_items_unpacked = torch.cat((displayed_items_unpacked.to(self.device), not_clicking_feature_vec.to(self.device)), -2) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1), feature_dims]\n",
        "        displayed_items_flat = displayed_items_unpacked.view(batch_size, num_time_steps, -1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)*feature_dims]\n",
        "        input_features = torch.cat((displayed_items_flat, state_unpacked), dim=-1) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items*feature_dims) + state_dim]\n",
        "        \n",
        "        return self.model(input_features) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "\n",
        "\n",
        "    def get_index(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state (torch.Tensor): [num_time_steps, state_dim]\n",
        "            displayed_items (torch.Tensor): [num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            generated_action_indices (torch.Tensor): [batch_size (#users), num_time_steps] indices of the actions chosen from the displayed_items by the user model\n",
        "            # Note that (num_displayed_items+1)^th index refers to the user not clickin on any of the items (i.e. zero feature vector)\n",
        "        \"\"\"\n",
        "        out = self.forward(state, displayed_items) # --> [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "        # find the action with the highest probability\n",
        "        pred_probs = torch.nn.functional.softmax(out, dim=2) # --> [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "        generated_action_indices = torch.argmax(pred_probs, dim=2) # --> [batch_size (#users), num_time_steps]\n",
        "        \n",
        "        return generated_action_indices\n",
        "\n",
        "    def get_corresponding_feature_vec(self, generated_action_indices, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            generated_action_indices (torch.Tensor): [batch_size (#users), num_time_steps] indices of the actions chosen from the displayed_items by the user model\n",
        "            # ! Note that (num_displayed_items+1)^th index refers to the user not clickin on any of the items (i.e. zero feature vector) !\n",
        "            displayed_items (torch.Tensor): [num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            generated_action_vectors (torch.Tensor): corresponding feature vectors of the generated actions specified with the generated_action_indices \n",
        "                [batch_size (#users), num_time_steps, feature_dims]\n",
        "        \"\"\"\n",
        "        # Convert rnn.PackedSequences to simple Tensors\n",
        "        displayed_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(displayed_items, batch_first=True)\n",
        "        \n",
        "        # Prepare input\n",
        "        batch_size = generated_action_indices.shape[0] # B\n",
        "        num_time_steps = displayed_items_unpacked.shape[1] # L\n",
        "        # Handle (num_displayed_items+1)^th index which refers to the user not clickin on any of the items (i.e. zero feature vector)\n",
        "        not_clicking_feature_vec = torch.zeros((batch_size, num_time_steps, 1, displayed_items_unpacked.shape[-1])).to(self.device) # --> [batch_size (#users), max(num_time_steps), 1, feature_dim]\n",
        "        displayed_items_unpacked = torch.cat((displayed_items_unpacked.to(self.device), not_clicking_feature_vec), -2) # --> [batch_size (#users), num_time_steps, (num_displayed_items+1), feature_dims]\n",
        "\n",
        "        # Extract the feature vectors that correspond to the generated action indices\n",
        "        #TODO implement this faster\n",
        "        generated_action_vectors = torch.zeros((generated_action_indices.shape[0], generated_action_indices.shape[1], displayed_items_unpacked.shape[-1])) # --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "        for b in range(generated_action_indices.shape[0]): # batch index\n",
        "            for t in range(generated_action_indices.shape[1]): # time step index\n",
        "                chosen_index = generated_action_indices[b, t]\n",
        "                generated_action_vectors[b, t, :] = displayed_items_unpacked[b, t, chosen_index, :]\n",
        "\n",
        "        return generated_action_vectors # --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "\n",
        "\n",
        "    def generate_actions(self, state, displayed_items):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state (torch.Tensor): [num_time_steps, state_dim]\n",
        "            displayed_items (torch.Tensor): [num_time_steps, num_displayed_items, feature_dims]\n",
        "        Return:\n",
        "            generated_action_indices (torch.Tensor): indices of the chosen actions. [batch_size (#users), num_time_steps, (num_displayed_items+1)]\n",
        "            generated_action_vectors (torch.Tensor): corresponding feature vectors of the generated actions specified with the generated_action_indices \n",
        "                [batch_size (#users), num_time_steps, feature_dims]\n",
        "        \"\"\"\n",
        "        # Obtain indices of the actions chosen from the display set\n",
        "        generated_action_indices = self.get_index(state, displayed_items) # --> [batch_size (#users), num_time_steps]\n",
        "        # Obtain action feature vectors corresponding to the indices of the generated actions\n",
        "        generated_action_vectors = self.get_corresponding_feature_vec(generated_action_indices, displayed_items) # --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "\n",
        "        return generated_action_indices , generated_action_vectors #[batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d7706f-1ac5-4d34-ab48-e3b8053d2373",
      "metadata": {
        "id": "17d7706f-1ac5-4d34-ab48-e3b8053d2373"
      },
      "source": [
        "HistoryLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a559c73d-2fb1-4dad-bd68-30a86ed2b9ff",
      "metadata": {
        "id": "a559c73d-2fb1-4dad-bd68-30a86ed2b9ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# This model takes in the old state and the newly chosen action as input and produces the new state representation\n",
        "# using LSTM(Long Short Term Memory)\n",
        "class History_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        \"\"\"\n",
        "        input_size (int): feature_dim of the actions.\n",
        "        hidden_size (int): dimension of the state representation vector (dim of output)\n",
        "        num_layers (int): number of recurrent layers in the LSTM model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.num_layers = num_layers\n",
        "        self.state_dim = hidden_size\n",
        "        self.lstm_model = torch.nn.LSTM(input_size, self.state_dim, self.num_layers, batch_first=True).to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, actions):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            new_action (torch.Tensor): action chosen by the user (either ground truth action or Generator_UserModel generated action).\n",
        "            [batch_size (#users), num_time_steps, feature_dim]\n",
        "        Returns:\n",
        "            new_state (torch.Tensor): old_state updated after taking new_action (i.e. updated history representation). \n",
        "            [batch_size (#users), num_time_steps, state_dim]\n",
        "            (h, c) (tuple): hidden and cell states. \n",
        "            Note that the returned new_state tensor is of same shape as the old_state tensor. \n",
        "        \"\"\"\n",
        "        out, _ = self.lstm_model(actions)\n",
        "        return out \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a471665f-b5a9-4820-9411-5e8f216dbd32",
      "metadata": {
        "id": "a471665f-b5a9-4820-9411-5e8f216dbd32"
      },
      "source": [
        "GAN Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "21a7006b-2a7c-419e-a6b1-120c91331caf",
      "metadata": {
        "id": "21a7006b-2a7c-419e-a6b1-120c91331caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8aba9f7-6658-4137-b138-cd4d3276176b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## ========================================================== DEBUG \\nif __name__ == \"__main__\":\\n    gan = GAN() #TODO: pass in constructor parameters\\n    gan.gan_training_loop(train_loader, validation_loader)\\n    # gan.test() #TODO\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from distutils.command.config import config\n",
        "import torch\n",
        "import numpy as np\n",
        "# import custom models\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_results(dreal_losses, dfake_losses, val_dreal_losses, val_dfake_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1, len(dreal_losses)+1)), dreal_losses, marker='o', label=\"dreal_loss\")\n",
        "    plt.plot(list(range(1, len(dfake_losses)+1)), dfake_losses, marker='*', label=\"dfake_loss\")\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(\"results/Training Losses Plot\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1, len(val_dreal_losses)+1)), val_dreal_losses, marker='o', label=\"dreal_loss\")\n",
        "    plt.plot(list(range(1, len(val_dfake_losses)+1)), val_dfake_losses, marker='*', label=\"dfake_loss\")\n",
        "    plt.title(\"Validation Losses\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(\"results/Validation Losses Plot\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Note that GAN is a model which orchestrated the mini-max game (training) between the  discriminator and the  generator model.\n",
        "class GAN():\n",
        "    def __init__(self, config_dict, history_input_size, history_hidden_size, history_num_layers, \\\n",
        "        generator_input_size, generator_output_size, generator_n_hidden, generator_hidden_dim, \\\n",
        "            discriminator_input_size, discriminator_output_size, discriminator_n_hidden, discriminator_hidden_dim, \\\n",
        "                lr=0.0006, betas=[0.3,0.999], epochs=150):        \n",
        "        \"\"\"\n",
        "        == Parameters of the History_LSTM:\n",
        "            history_input_size (int): feature_dim of the actions.\n",
        "            history_hidden_size (int): dimension of the state representation vector (dim of output of the History_LSTM)\n",
        "            history_num_layers (int): number of recurrent layers in the History_LSTM.\n",
        "\n",
        "        == Parameters of the Generator_UserModel:\n",
        "            generator_input_size (int): equals ((num_displayed_items+1)*feature_dims + state_dim)\n",
        "            generator_output_size (int): equals (num_displayed_items+1)\n",
        "            generator_n_hidden (int): number of hidden layers in the generator model.\n",
        "            generator_hidden_dim (int): hidden dimension of the layers in the generator model.\n",
        "\n",
        "        == Parameters of teh Discriminator_RewardModel:\n",
        "            discriminator_input_size (int): should equal (num_displayed_items*feature_dims) + state_dim.\n",
        "            discriminator_output_size (int): should equal (num_displayed_items+1). \n",
        "            discriminator_n_hidden (int): number of hidden layers of the Discriminator model's MLP.\n",
        "            discriminator_hidden_dim (int): hidden dimension of the layers of the Discriminator model's MLP.\n",
        "        \n",
        "        == Hyperparameters of the training\n",
        "            lr (int): learning rate used by the optimizer.\n",
        "            betas (tuple): beta values used by the ADAM optimizer.\n",
        "            epochs (int): number of epochs to train.\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.history_LSTM = History_LSTM(history_input_size, history_hidden_size, history_num_layers).to(self.device)\n",
        "        self.generator_UserModel = Generator_UserModel(generator_input_size, generator_output_size, generator_n_hidden, generator_hidden_dim).to(self.device)\n",
        "        self.discriminator_RewardModel = Discriminator_RewardModel(discriminator_input_size, discriminator_output_size, discriminator_n_hidden, discriminator_hidden_dim).to(self.device)\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.epochs = epochs\n",
        "        self.config_dict = config_dict\n",
        "        \n",
        "    \n",
        "    def gan_training_loop(self, train_loader, validation_loader):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            train_loader (torch.Tensor): training DataLoader\n",
        "            test_loader (torch.Tensor): training DataLoader \n",
        "        Return:\n",
        "            generated_actions (torch.tensor): Actions taken by the generator_UserModel.\n",
        "            UserModel_rewards (torch.tensor): Reward values for the generator_UserModel generated actions.\n",
        "            ground_truth_rewards (torch.tensor): Reward values for the ground truth actions.\n",
        "        \"\"\"\n",
        "        history_LSTM_optimizer = torch.optim.Adam(self.history_LSTM.parameters(), lr=self.lr, betas=self.betas)\n",
        "        discriminator_optimizer = torch.optim.Adam(self.discriminator_RewardModel.parameters(), lr=self.lr, betas=self.betas)\n",
        "        generator_optimizer = torch.optim.Adam(self.generator_UserModel.parameters(), lr=self.lr, betas=self.betas)\n",
        "\n",
        "\n",
        "        # ============= Load models from ckpts\n",
        "        loaded_epoch = 0\n",
        "        dreal_loaded_loss = None\n",
        "        dfake_loaded_loss = None\n",
        "        if self.config_dict[\"load_pretrained\"]:\n",
        "            history_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_history_lstm_path\"]))\n",
        "            generator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_generator_path\"]))\n",
        "            discriminator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_discriminator_path\"]))\n",
        "\n",
        "            self.history_LSTM.load_state_dict(history_ckpt[\"state_dict\"])\n",
        "            self.generator_UserModel.load_state_dict(generator_ckpt[\"state_dict\"])\n",
        "            self.discriminator_RewardModel.load_state_dict(discriminator_ckpt[\"state_dict\"])\n",
        "\n",
        "            history_LSTM_optimizer.load_state_dict(history_ckpt[\"optimizer_state_dict\"])\n",
        "            discriminator_optimizer.load_state_dict(discriminator_ckpt[\"optimizer_state_dict\"])\n",
        "            generator_optimizer.load_state_dict(generator_ckpt[\"optimizer_state_dict\"])\n",
        "\n",
        "            loaded_epoch = generator_ckpt[\"epoch\"]\n",
        "            dreal_loaded_loss = generator_ckpt[\"dreal_loss\"]\n",
        "            dfake_loaded_loss = generator_ckpt[\"dfake_loss\"]\n",
        "            print(f\"Loaded History_lstm, Discriminator, and Generator from saved ckpt. Loaded epoch:{loaded_epoch}, \\\n",
        "                Loaded best real validation loss: {dreal_loaded_loss}, Loaded best fkae validation loss: {dfake_loaded_loss}\")\n",
        "        # ================\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize empty lists to hold the generator and discriminator losses\n",
        "        dfake_losses = [] # training losses\n",
        "        dreal_losses = []\n",
        "        val_dfake_losses = [] # validation losses\n",
        "        val_dreal_losses = []\n",
        "\n",
        "        print(\"*\" * 30)\n",
        "        print(\"Training GAN Model\")\n",
        "        print(\"*\" * 30)\n",
        "\n",
        "        for epoch in range(self.epochs - loaded_epoch): \n",
        "            dreal_best_val_loss = None if dreal_loaded_loss == None else dreal_loaded_loss # best validation loss (used during saving checkpoints)\n",
        "            dfake_best_val_loss = None if dfake_loaded_loss == None else dfake_loaded_loss # best validation loss (used during saving checkpoints)\n",
        "            cur_dreal_loss = 0 # total loss for cur batch\n",
        "            cur_dfake_loss = 0 # total loss for cur batch\n",
        "            for real_click_history, display_set, clicked_items  in train_loader:\n",
        "                # real_click_history --> [max(num_time_steps), feature_dim]\n",
        "                # display_set --> [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "                # clicked_items --> [max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "                 \n",
        "                real_click_history = real_click_history.to(self.device)\n",
        "                display_set = display_set.to(self.device)\n",
        "                clicked_items = clicked_items.to(self.device)\n",
        "\n",
        "                # Updating the discriminator, here is a pseudocode        \n",
        "                # call zero grad\n",
        "                # pass the real actions through D\n",
        "                # calculate d_real loss\n",
        "                # generate fake user actions\n",
        "                # pass the generated_user_actions through D\n",
        "                # calculate d_fake loss\n",
        "                # sum the two losses\n",
        "                # call backward and take optimizer step\n",
        "\n",
        "\n",
        "\n",
        "                # ************************************ discriminator_RewardModel Loss Calculation below: ************************************\n",
        "\n",
        "                # Obtain state representations given the real user's past click history\n",
        "                real_states = self.history_LSTM(real_click_history) # --> [batch_size (#users)=1, num_time_steps, state_dim]\n",
        "                # Calculate the rewards for all of the possible actions (items in the (display_set+1))\n",
        "                dreal_reward = self.discriminator_RewardModel.forward(real_states, display_set) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                \n",
        "                # Calculate the rewards for the real user actions by masking by the actions taken by the real user\n",
        "                class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                clicked_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                clicked_item_mask = torch.nn.functional.one_hot(clicked_items_unpacked.long(), num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                gt_reward = dreal_reward * clicked_item_mask.float()\n",
        "                _, total_unpadded_num_time_steps = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                dreal_loss = torch.sum(gt_reward) / sum(total_unpadded_num_time_steps) # avg loss/rewards for the real user actions (gt)\n",
        "\n",
        "\n",
        "\n",
        "                # ========== generator_UserModel Loss Calculation below: \n",
        "                # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                with torch.no_grad():\n",
        "                    generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                # convert rnn.PackedSequence to Tensor\n",
        "                real_click_history_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                    for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                        cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                        cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                        # append generated action to past history from the real user\n",
        "                        cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                        cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                        # obtain new state representations after taking the current generated action\n",
        "                        cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "\n",
        "                        # calculate the reward for the currently generated action\n",
        "                        display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                        cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                        # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                        cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                        \n",
        "                        class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                        cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        \n",
        "                        cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                \n",
        "                dfake_loss = gen_reward # total loss/rewards for the real user actions (gt)\n",
        "\n",
        "                # Update Disciriminator (Reward) model\n",
        "                # ============ loss backpropagation:\n",
        "                combined_loss = dfake_loss - dreal_loss\n",
        "                if combined_loss.requires_grad:\n",
        "                    # Backprop discriminator_RewardModel\n",
        "                    # Note that discriminator_RewardModel tries to minimize the combined_loss\n",
        "                    for param in self.discriminator_RewardModel.parameters():\n",
        "                        param.requires_grad = True\n",
        "                    for param in self.generator_UserModel.parameters():\n",
        "                        param.requires_grad = False\n",
        "                    history_LSTM_optimizer.zero_grad()\n",
        "                    generator_optimizer.zero_grad()\n",
        "                    discriminator_optimizer.zero_grad()\n",
        "                    combined_loss.backward()\n",
        "                    history_LSTM_optimizer.step()\n",
        "                    discriminator_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # ************************************ generator_UserModel Loss Calculation below: ************************************\n",
        "                # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                # convert rnn.PackedSequence to Tensor\n",
        "                # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                    for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                        cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                        cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                        # append generated action to past history from the real user\n",
        "                        cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                        cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                        # obtain new state representations after taking the current generated action\n",
        "                        cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "                        # calculate the reward for the currently generated action\n",
        "                        display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                        \n",
        "                        cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                        # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                        cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                        \n",
        "                        class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                        clicked_item_mask = torch.nn.functional.one_hot(clicked_items.long().data, num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                        cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        \n",
        "                        cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                \n",
        "                dfake_loss = -1 * gen_reward # total loss/rewards for the real user actions (gt)\n",
        "                \n",
        "                # ============ loss backpropagation:\n",
        "                combined_loss = dfake_loss\n",
        "                if combined_loss.requires_grad:\n",
        "                    # backprop generator_UserModel\n",
        "                    # Note that generator_UserModel tries to maximize the combined_loss\n",
        "                    for param in self.generator_UserModel.parameters():\n",
        "                        param.requires_grad = True\n",
        "                    for param in self.discriminator_RewardModel.parameters():\n",
        "                        param.requires_grad = False\n",
        "                    history_LSTM_optimizer.zero_grad()\n",
        "                    generator_optimizer.zero_grad()\n",
        "                    discriminator_optimizer.zero_grad()\n",
        "                    combined_loss.backward()\n",
        "                    history_LSTM_optimizer.step()\n",
        "                    generator_optimizer.step()\n",
        "\n",
        "                    # record losses\n",
        "                    cur_dfake_loss += dfake_loss.detach().cpu().numpy()\n",
        "                    cur_dreal_loss += dreal_loss.detach().cpu().numpy()\n",
        "\n",
        "            # logging\n",
        "            dreal_losses.append(cur_dreal_loss)\n",
        "            dfake_losses.append(cur_dfake_loss)\n",
        "\n",
        "        \n",
        "\n",
        "            # ================== Validation part\n",
        "            val_cur_dreal_loss = 0 # total loss for cur batch\n",
        "            val_cur_dfake_loss = 0 # total loss for cur batch\n",
        "            for real_click_history, display_set, clicked_items  in validation_loader:\n",
        "                # real_click_history --> [max(num_time_steps), feature_dim]\n",
        "                # display_set --> [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "                # clicked_items --> [max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "                \n",
        "                real_click_history = real_click_history.to(self.device)\n",
        "                display_set = display_set.to(self.device)\n",
        "                clicked_items = clicked_items.to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                     # Obtain state representations given the real user's past click history\n",
        "                    real_states = self.history_LSTM(real_click_history) # --> [batch_size (#users)=1, num_time_steps, state_dim]\n",
        "                    # Calculate the rewards for all of the possible actions (items in the (display_set+1))\n",
        "                    dreal_reward = self.discriminator_RewardModel.forward(real_states, display_set) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                    \n",
        "                    # Calculate the rewards for the real user actions by masking by the actions taken by the real user\n",
        "                    class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                    clicked_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                    clicked_item_mask = torch.nn.functional.one_hot(clicked_items_unpacked.long(), num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                    gt_reward = dreal_reward * clicked_item_mask.float()\n",
        "                    dreal_loss = torch.sum(gt_reward) / dreal_reward.shape[1] # avg loss/rewards for the real user actions (gt)\n",
        "\n",
        "\n",
        "\n",
        "                    # ========== generator_UserModel Loss Calculation below: \n",
        "                    # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                    generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                    # convert rnn.PackedSequence to Tensor\n",
        "                    real_click_history_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                    # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                    gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                    for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                        for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                            cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                            cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                            # append generated action to past history from the real user\n",
        "                            cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                            cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                            # obtain new state representations after taking the current generated action\n",
        "                            cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "                            # calculate the reward for the currently generated action\n",
        "                            display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                            cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                            cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                            # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                            cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                            \n",
        "                            class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                            cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                            \n",
        "                            cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                            gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                    \n",
        "                    dfake_loss = -1 * gen_reward # total loss/rewards for the real user actions (gt)\n",
        "\n",
        "                    # record losses\n",
        "                    val_cur_dfake_loss += dfake_loss.detach().cpu().numpy()\n",
        "                    val_cur_dreal_loss += dreal_loss.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "            # =========== Save Checkpoints\n",
        "            if (dfake_best_val_loss == None) or (dfake_best_val_loss >= val_cur_dfake_loss):\n",
        "                if not os.path.exists(self.config_dict[\"ckpt_path\"]):\n",
        "                    os.mkdir(self.config_dict[\"ckpt_path\"])\n",
        "\n",
        "                dfake_best_val_loss = val_cur_dfake_loss\n",
        "                # Save history_lstm\n",
        "                torch.save({\n",
        "                    'epoch': epoch + loaded_epoch,\n",
        "                    'state_dict': self.history_LSTM.state_dict(),\n",
        "                    'optimizer_state_dict': history_LSTM_optimizer.state_dict(),\n",
        "                    'dfake_loss': dfake_best_val_loss,\n",
        "                    'dreal_loss': val_cur_dreal_loss,\n",
        "                }, os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_history_lstm_path\"]))\n",
        "\n",
        "                # Save Generator\n",
        "                torch.save({\n",
        "                    'epoch': epoch + loaded_epoch,\n",
        "                    'state_dict': self.generator_UserModel.state_dict(),\n",
        "                    'optimizer_state_dict': generator_optimizer.state_dict(),\n",
        "                    'dfake_loss': dfake_best_val_loss,\n",
        "                    'dreal_loss': val_cur_dreal_loss,\n",
        "                }, os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_generator_path\"]))\n",
        "\n",
        "                # Save Discriminator\n",
        "                torch.save({\n",
        "                    'epoch': epoch + loaded_epoch,\n",
        "                    'state_dict': self.discriminator_RewardModel.state_dict(),\n",
        "                    'optimizer_state_dict': discriminator_optimizer.state_dict(),\n",
        "                    'dreal_loss': dfake_best_val_loss,\n",
        "                    'dreal_loss': val_cur_dreal_loss,\n",
        "                }, os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_discriminator_path\"]))\n",
        "\n",
        "                print(\"*\" * 20)\n",
        "                print(f\"Saved model checkpoint at epoch: {epoch+loaded_epoch}\")\n",
        "            \n",
        "            # logging\n",
        "            val_dreal_losses.append(val_cur_dreal_loss)\n",
        "            val_dfake_losses.append(val_cur_dfake_loss)\n",
        "\n",
        "            print(\"_\" * 25)\n",
        "            print(f\"epoch: [{epoch+1+loaded_epoch}/{self.epochs}], train_dreal_loss: {dreal_losses[-1]}, train_dfake_loss: {dfake_losses[-1]} \\\n",
        "                val_dreal_loss: {val_dreal_losses[-1]}, val_dfake_loss: {val_dfake_losses[-1]}\")\n",
        "            print(\"_\" * 25)\n",
        "\n",
        "        plot_results(dreal_losses, dfake_losses, val_dreal_losses, val_dfake_losses)\n",
        "        # Return the losses\n",
        "        return dreal_losses, dfake_losses, val_dreal_losses, val_dfake_losses\n",
        "\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        print(\"*\" * 30)\n",
        "        print(\"Testing GAN Model\")\n",
        "        print(\"*\" * 30)\n",
        "\n",
        "        # ================== Load ckpt\n",
        "        if self.config_dict[\"load_pretrained\"]:\n",
        "            history_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_history_lstm_path\"]))\n",
        "            generator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_generator_path\"]))\n",
        "            discriminator_ckpt = torch.load(os.path.join(self.config_dict[\"ckpt_path\"], self.config_dict[\"pretrained_discriminator_path\"]))\n",
        "\n",
        "            self.history_LSTM.load_state_dict(history_ckpt[\"state_dict\"])\n",
        "            self.generator_UserModel.load_state_dict(generator_ckpt[\"state_dict\"])\n",
        "            self.discriminator_RewardModel.load_state_dict(discriminator_ckpt[\"state_dict\"])\n",
        "\n",
        "           \n",
        "            loaded_epoch = generator_ckpt[\"epoch\"]\n",
        "            dreal_loaded_loss = generator_ckpt[\"dreal_loss\"]\n",
        "            dfake_loaded_loss = generator_ckpt[\"dfake_loss\"]\n",
        "            print(f\"Loaded History_lstm, Discriminator, and Generator from saved ckpt. Loaded epoch:{loaded_epoch}, \\\n",
        "                Loaded best real validation loss: {dreal_loaded_loss}, Loaded best fake validation loss: {dfake_loaded_loss}\")\n",
        "        # ==================\n",
        "\n",
        "        top_k_precisions_list = [] # top k@precision \n",
        "        generator_precision = [[]] # only top 1@prec scoker\n",
        "        for k in self.config_dict[\"k\"]: # initialize list\n",
        "            top_k_precisions_list.append(list())\n",
        "                \n",
        "        test_cur_dreal_loss = 0 # total loss for cur batch\n",
        "        test_cur_dfake_loss = 0 # total loss for cur batch\n",
        "        \n",
        "        for real_click_history, display_set, clicked_items  in test_dataloader:\n",
        "            # real_click_history --> [max(num_time_steps), feature_dim]\n",
        "            # display_set --> [max(num_time_steps), num_displayed_item, feature_dim]\n",
        "            # clicked_items --> [max(num_time_steps)] display set index of the clicked items by the real user (gt user actions)\n",
        "            \n",
        "            real_click_history = real_click_history.to(self.device)\n",
        "            display_set = display_set.to(self.device)\n",
        "            clicked_items = clicked_items.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Obtain state representations given the real user's past click history\n",
        "                real_states = self.history_LSTM(real_click_history) # --> [batch_size (#users)=1, num_time_steps, state_dim]\n",
        "                # Calculate the rewards for all of the possible actions (items in the (display_set+1))\n",
        "                dreal_reward = self.discriminator_RewardModel.forward(real_states, display_set) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                \n",
        "                \n",
        "                # ===============\n",
        "                # Find unpadded indices of the displayed_set\n",
        "                unpacked_displayed_items, lens_displayed_item = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                unpacked_displayed_items = unpacked_displayed_items.to(self.device)\n",
        "                lens_displayed_item = lens_displayed_item.to(self.device)\n",
        "                # unpacked_displayed_items --> [B, max(num_time_steps), padded_display_set, feature_dim]\n",
        "                display_unpadded_indices = []\n",
        "                for b in range(unpacked_displayed_items.shape[0]):\n",
        "                    cur_l_indices = []\n",
        "                    for l in range(unpacked_displayed_items.shape[1]):\n",
        "                        cur_t_indices = []\n",
        "                        for i, item in enumerate(unpacked_displayed_items[b,l, :]):\n",
        "                            # item --> [feature_dim]\n",
        "                            if item.sum() != item.shape[-1]: # input is a padding vector for displayed_items (full 1 Tensor [1,1,1 ..., 1])\n",
        "                                cur_t_indices.append(i)\n",
        "                        cur_t_indices.append(len(unpacked_displayed_items[b,l, :])) # append non_clicking vector\n",
        "                        cur_l_indices.append(cur_t_indices)\n",
        "                                \n",
        "                    display_unpadded_indices.append(cur_l_indices)\n",
        "                \n",
        "                # display_unpadded_indices --> [B, l, value]\n",
        "\n",
        "\n",
        "                # Find max k indices which are not padded in the displayed_items\n",
        "                # dreal_reward --> [B, l, max(num_displayed_item)]\n",
        "                # unpacked_clicked_items --> [B, l]\n",
        "                # lens_clicked_item --> [l]\n",
        "                unpacked_clicked_items, lens_clicked_item = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                unpacked_clicked_items =  unpacked_clicked_items.to(self.device)\n",
        "                lens_clicked_item = lens_clicked_item.to(self.device)\n",
        "                for k in self.config_dict[\"k\"]:\n",
        "                    precision_list = []\n",
        "                    for b in range(dreal_reward.shape[0]):\n",
        "                        for l in range(lens_clicked_item[b]):\n",
        "                            # dreal_reward[b,l,:] --> 11\n",
        "                            # find max k indices\n",
        "                            unpadded_display_set = torch.gather(dreal_reward[b, l, :], 0, torch.tensor(display_unpadded_indices[b][l]).to(self.device))\n",
        "                            # chose max k from unpadded_display_set\n",
        "                            _, top_k_pred = torch.topk(unpadded_display_set, k)\n",
        "                            \n",
        "                            top_k_pred = top_k_pred.tolist()\n",
        "                            # find real user's choice index\n",
        "                            real_indices = unpacked_clicked_items[b, l]\n",
        "                            \n",
        "                            if real_indices in top_k_pred: # if the discriminator guessed right\n",
        "                                precision_list.append(1)\n",
        "                            else: # if the discriminator couldn't guess right\n",
        "                                precision_list.append(0)\n",
        "                    \n",
        "                    top_k_precisions_list[k-1].extend(precision_list)\n",
        "                # =====================\n",
        "                \n",
        "                \n",
        "\n",
        "                # Calculate the rewards for the real user actions by masking by the actions taken by the real user\n",
        "                class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                \n",
        "\n",
        "                clicked_items_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(clicked_items, batch_first=True)\n",
        "                clicked_item_mask = torch.nn.functional.one_hot(clicked_items_unpacked.long(), num_classes= class_num) # --> [batch_size (#users), max(num_time_steps), (num_displayed_items+1)]\n",
        "                gt_reward = dreal_reward * clicked_item_mask.float()\n",
        "                dreal_loss = torch.sum(gt_reward) / dreal_reward.shape[1] # avg loss/rewards for the real user actions (gt)\n",
        "\n",
        "\n",
        "\n",
        "                # ========== generator_UserModel top-k@Precision Calculation below: \n",
        "                # Obtain generated user action's indices/feature vectors for 1 time step ahead given the past real users state representation\n",
        "                # convert rnn.PackedSequence to Tensor\n",
        "                generated_action_indices , generated_action_vectors = self.generator_UserModel.generate_actions(real_states, display_set)  # --> [batch_size (#users), num_time_steps] , [batch_size (#users), num_time_steps, feature_dims]\n",
        "                # generated_action_indices --> [B, L] index of the best chosen action\n",
        "                \n",
        "                generator_precision_list = []\n",
        "                for b in range(generated_action_indices.shape[0]):\n",
        "                    for l in range(lens_clicked_item[b]):\n",
        "                            # dreal_reward[b,l,:] --> 11\n",
        "                            # find max k indices\n",
        "                        top_1_pred = generated_action_indices[b, l]\n",
        "                        top_1_pred = [top_1_pred.tolist()]\n",
        "\n",
        "                        # find real user's choice index\n",
        "                        real_indices = unpacked_clicked_items[b, l]\n",
        "                            \n",
        "                        if real_indices in top_1_pred: # if the discriminator guessed right\n",
        "                            generator_precision_list.append(1)\n",
        "                        else: # if the discriminator couldn't guess right\n",
        "                            generator_precision_list.append(0)\n",
        "                    \n",
        "                generator_precision[0].extend(generator_precision_list)\n",
        "                \n",
        "                \n",
        "                \n",
        "                real_click_history_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(real_click_history, batch_first=True)\n",
        "                # generated_action_vectors --> [batch_size (#users), num_time_steps, feature_dims]\n",
        "                gen_reward = torch.tensor(0).float().to(self.device)\n",
        "                \n",
        "                \n",
        "                for b in range(generated_action_vectors.shape[0]): # index on batch_size\n",
        "                    for t in range(1, generated_action_vectors.shape[1]): # index on num_time_steps (L)\n",
        "                        cur_generated_action_vector = generated_action_vectors[b, t, :].to(self.device) # --> [feature_dim]\n",
        "                        cur_real_past_actions = real_click_history_unpacked[b, :t, :].to(self.device) # --> [t, feature_dim]\n",
        "                        # append generated action to past history from the real user\n",
        "                        cur_generated_action_with_history = torch.cat((cur_real_past_actions, cur_generated_action_vector.unsqueeze(0)), dim=0) # --> [t+1, feature_dim]\n",
        "                        cur_generated_action_with_history = cur_generated_action_with_history.unsqueeze(0) # --> [1, t+1, feature_dim]\n",
        "                        # obtain new state representations after taking the current generated action\n",
        "                        cur_fake_state = self.history_LSTM(cur_generated_action_with_history) # --> [1, t+1, state_dim]\n",
        "                        # calculate the reward for the currently generated action\n",
        "                        display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        cur_display_set = display_set_unpacked[b, :t+1, :, :].unsqueeze(0) # --> [1, t+1, num_displayed_item, feature_dim]\n",
        "                        cur_dfake_reward = self.discriminator_RewardModel(cur_fake_state, cur_display_set) # --> [1, t+1, (num_displayed_items+1)]\n",
        "\n",
        "                        # Calculate the rewards for the generated user actions by masking by the generated rewards for all of the possible acitons in the display_set\n",
        "                        cur_generated_action_indices = generated_action_indices[b, :t+1].unsqueeze(0) # --> [1, t+1]\n",
        "                        \n",
        "                        class_num = ((display_set.data.shape[1])+1) # (num_displayed_items+1)\n",
        "                        cur_clicked_item_mask = torch.nn.functional.one_hot(cur_generated_action_indices, num_classes= class_num) # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        \n",
        "                        cur_gen_reward = cur_dfake_reward * cur_clicked_item_mask.float() # --> [1, t+1, (num_displayed_items+1)]\n",
        "                        gen_reward += torch.sum(cur_gen_reward) / cur_gen_reward.shape[1]\n",
        "                \n",
        "                dfake_loss = -1 * gen_reward # total loss/rewards for the real user actions (gt)\n",
        "\n",
        "                # record losses\n",
        "                test_cur_dfake_loss += dfake_loss.detach().cpu().numpy()\n",
        "                test_cur_dreal_loss += dreal_loss.detach().cpu().numpy()\n",
        "        \n",
        "        \n",
        "        # calculate top k@prec\n",
        "        top_k_precicions = []\n",
        "        for e in top_k_precisions_list:\n",
        "            top_k_precicions.append(sum(e)/len(e))\n",
        "\n",
        "        padded_display_set_size = self.config_dict[\"generator_output_size\"]\n",
        "        print(\"*\"*10)\n",
        "        print(f\"Padded Display set size = {padded_display_set_size}\")\n",
        "        for k in self.config_dict[\"k\"]:\n",
        "            print(f\"Greedy Discriminator Reward Model Prec@{k} = {top_k_precicions[k-1]}\")\n",
        "\n",
        "        print(f\"Generator User Model Prec@1 = {np.mean(generator_precision[0])}\")\n",
        "        print(\"*\"*10)\n",
        "                \n",
        "\n",
        "        print(f\"test_cur_dfake_loss: {test_cur_dfake_loss}, test_cur_dreal_loss: {test_cur_dreal_loss}\")\n",
        "        print(\"_\" * 25)\n",
        "\n",
        "        return test_cur_dreal_loss, test_cur_dfake_loss\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## ========================================================== DEBUG \n",
        "if __name__ == \"__main__\":\n",
        "    gan = GAN() #TODO: pass in constructor parameters\n",
        "    gan.gan_training_loop(train_loader, validation_loader)\n",
        "    # gan.test() #TODO\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711611ea-480a-4669-997d-57abdb8befc3",
      "metadata": {
        "id": "711611ea-480a-4669-997d-57abdb8befc3"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e14ea2aa-beda-428b-a4bd-24aa8d5f262f",
      "metadata": {
        "id": "e14ea2aa-beda-428b-a4bd-24aa8d5f262f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "17f0b6b9-3344-46c9-9b04-74892af0fbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************\n",
            "Training GAN Model\n",
            "******************************\n",
            "********************\n",
            "Saved model checkpoint at epoch: 0\n",
            "_________________________\n",
            "epoch: [1/2], train_dreal_loss: 26.1603640106041, train_dfake_loss: 575.0713365077972                 val_dreal_loss: 275.12342643737793, val_dfake_loss: 704.0\n",
            "_________________________\n",
            "********************\n",
            "Saved model checkpoint at epoch: 1\n",
            "_________________________\n",
            "epoch: [2/2], train_dreal_loss: 69.38009411096573, train_dfake_loss: 1024.0                 val_dreal_loss: 275.12357902526855, val_dfake_loss: 704.0\n",
            "_________________________\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "import yaml\n",
        "from copy import deepcopy\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_dataLoaders(data_folder, dset, batch_size):\n",
        "    # Initialize Dataloaders\n",
        "    train_dataset = Dataset(data_folder, dset, split=\"train\")\n",
        "    val_dataset = Dataset(data_folder, dset, split=\"validation\")\n",
        "    test_dataset = Dataset(data_folder, dset, split=\"test\")\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, drop_last=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "config_dict = {}\n",
        "\n",
        "mode = \"train\"\n",
        "\n",
        "# history_input_size: 804 # yelp = 804, rsc = 890, tb = 4042\n",
        "config_dict[\"history_hidden_size\"]= 512\n",
        "config_dict[\"history_num_layers\"]= 8\n",
        "\n",
        "# generator_input_size: 9356  # yelp = 9356, rsc = 10302, tb = 44974\n",
        "# generator_output_size: 11\n",
        "config_dict[\"generator_n_hidden\"]= 8\n",
        "config_dict[\"generator_hidden_dim\"]= 512 \n",
        "\n",
        "# discriminator_input_size: 9356 # yelp = 9356, rsc = 10302, tb = 44974\n",
        "# discriminator_output_size: 11\n",
        "config_dict[\"discriminator_n_hidden\"]= 8\n",
        "config_dict[\"discriminator_hidden_dim\"]= 512\n",
        "\n",
        "config_dict[\"lr\"]= 0.0006\n",
        "config_dict[\"betas\"]= [0.3,0.999]\n",
        "config_dict[\"epochs\"]= 2\n",
        "config_dict[\"batch_size\"]= 16\n",
        "config_dict[\"k\"]= [1, 2] # top k@precision's k values\n",
        "\n",
        "config_dict[\"load_pretrained\"]= False # load history_lstm, generator, and discrminator from checkpoints if given True\n",
        "config_dict[\"ckpt_path\"]= \"checkpoints\" # folder path to checkpoints\n",
        "config_dict[\"pretrained_history_lstm_path\"]= \"best_lstm_ckpt.pth.tar\" # history_lstm checkpoint to save/load model\n",
        "config_dict[\"pretrained_discriminator_path\"]= \"best_discriminator_ckpt.pth.tar\" # discriminator checkpoint to save/load model\n",
        "config_dict[\"pretrained_generator_path\"]= \"best_generator_ckpt.pth.tar\" # generator checkpoint to save/load model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         == Parameters of the History_LSTM:\n",
        "#             history_input_size (int): feature_dim of the actions.\n",
        "#             history_hidden_size (int): dimension of the state representation vector (dim of output of the History_LSTM)\n",
        "#             history_num_layers (int): number of recurrent layers in the History_LSTM.\n",
        "\n",
        "#         == Parameters of the Generator_UserModel:\n",
        "#             generator_input_size (int): equals ((num_displayed_items+1)*feature_dims + state_dim)\n",
        "#             generator_output_size (int): equals (num_displayed_items+1)\n",
        "#             generator_n_hidden (int): number of hidden layers in the generator model.\n",
        "#             generator_hidden_dim (int): hidden dimension of the layers in the generator model.\n",
        "\n",
        "#         == Parameters of teh Discriminator_RewardModel:\n",
        "#             discriminator_input_size (int): should equal (num_displayed_items*feature_dims) + state_dim.\n",
        "#             discriminator_output_size (int): should equal (num_displayed_items+1). \n",
        "#             discriminator_n_hidden (int): number of hidden layers of the Discriminator model's MLP.\n",
        "#             discriminator_hidden_dim (int): hidden dimension of the layers of the Discriminator model's MLP.\n",
        "        \n",
        "#         == Hyperparameters of the training\n",
        "#             lr (int): learning rate used by the optimizer.\n",
        "#             betas (tuple): beta values used by the ADAM optimizer.\n",
        "#             epochs (int): number of epochs to train. //\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize dataloaders\n",
        "data_folder = \"./dropbox\"\n",
        "dset = \"yelp\" # choose rsc, tb, or yelp\n",
        "assert dset in [\"yelp\", \"rsc\", \"tb\"]\n",
        "train_dataloader, val_dataloader, test_dataloader = get_dataLoaders(data_folder, dset, config_dict['batch_size'])\n",
        "\n",
        "if mode == \"train\":\n",
        "  real_click_history, display_set, clicked_items = next(iter(train_dataloader))\n",
        "  display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "elif mode == \"test\":\n",
        "  real_click_history, display_set, clicked_items = next(iter(test_dataloader))\n",
        "  display_set_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(display_set, batch_first=True)\n",
        "                        \n",
        "config_dict[\"generator_output_size\"] = display_set_unpacked.shape[-2] + 1\n",
        "config_dict[\"discriminator_output_size\"] = display_set_unpacked.shape[-2] + 1\n",
        "config_dict[\"history_input_size\"] = display_set_unpacked.shape[-1]\n",
        "config_dict[\"generator_input_size\"] = config_dict[\"history_hidden_size\"] + (config_dict[\"generator_output_size\"] * config_dict[\"history_input_size\"])\n",
        "config_dict[\"discriminator_input_size\"] = config_dict[\"history_hidden_size\"] + (config_dict[\"discriminator_output_size\"] * config_dict[\"history_input_size\"])\n",
        "\n",
        "  # Initialize the GAN model\n",
        "gan = GAN(config_dict, config_dict['history_input_size'], config_dict['history_hidden_size'], config_dict['history_num_layers'], \\\n",
        "        config_dict['generator_input_size'], config_dict['generator_output_size'], config_dict['generator_n_hidden'], config_dict['generator_hidden_dim'], \\\n",
        "            config_dict['discriminator_input_size'], config_dict['discriminator_output_size'], config_dict['discriminator_n_hidden'], config_dict['discriminator_hidden_dim'], \\\n",
        "                lr=config_dict['lr'], betas=config_dict['betas'], epochs=config_dict['epochs'])\n",
        "\n",
        "\n",
        "    # Train/Test using the GAN model\n",
        "if mode == \"train\":\n",
        "  train_dreal_losses, train_dfake_losses, val_dreal_losses, val_dfake_losses = gan.gan_training_loop(train_dataloader, val_dataloader)\n",
        "else:\n",
        "  test_cur_dreal_loss, test_cur_dfake_loss = gan.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34686866-053e-4969-a9ef-ac9761cf495e",
      "metadata": {
        "id": "34686866-053e-4969-a9ef-ac9761cf495e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0637cddd-6857-4b2c-84d3-886ae47ca56f",
      "metadata": {
        "id": "0637cddd-6857-4b2c-84d3-886ae47ca56f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "COMP 447 Final Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}